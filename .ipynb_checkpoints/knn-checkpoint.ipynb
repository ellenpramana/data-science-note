{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors (kNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Supervised learning**: need labeled training set\n",
    "* **Non-parametric**: not characterized by some fixed parameter, no assumptions on the underlying data distribution\n",
    "* **Instance Based Learning**: Store all training data, no explicit training phase or it's very minimal. This is also known as lazy learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Algorithm :\n",
    "\n",
    "#### Overview \n",
    "To classify an unlabeled data, the distance between this unlabeled data to the stored labeled data is computed. With the distance computed, k-nearest neighbors of this unlabeled data are identified, they are the closest to this data. A vote then is carried out to make decision, and determine which class label from the k-nearest neighbors will be assigned to the unlabeled data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "1. Given a training set D and a new unlabeled data $z = (\\mathbf{x}^*,y^*)$, $\\mathbf{x}$ is the vector data, while $y$ is the label or class. <br> <br>\n",
    "2. Calculate the distance between z = $(\\mathbf{x}^*,y^*)$ and all the training data $(\\mathbf{x}, y) \\in D $ <br>\n",
    "    Any distance can be used:\n",
    "    * Euclidean \n",
    "    * Cityblock\n",
    "    * Chebysev\n",
    "    * Manhanttan\n",
    "    * Minkowski\n",
    "<br>\n",
    "3. Select k number of data which is the closest to the new unlabeled data $z$, we name this subset $D_z$ <br> <br>\n",
    "4. Vote on labels. The unlabeled data $z$ is classified based on the majority class of its k-nearest neighbors. <br> <br>\n",
    "$$ y^* = \\underset{v}{\\arg\\max}\\ \\sum_{(x_i, y_i) \\in D_z} I(v = y_i) $$ <br> <br>\n",
    "where $v$ is a class label, $y_i$ is the class label for the $i$th nearest neighbors, and $I$ is an\n",
    "indicator function that returns the value 1 if its argument is true and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Illustration\n",
    "<img src = 'image/knn_algorithm.jpg' height=\"600\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The choice of k. <br>\n",
    "   k is too small -> sensitive to noise points (overfit) <br>\n",
    "   k is too large -> neigbhorhood might include too many points from other classes (underfit)\n",
    "   <br> <br>\n",
    "2. Curse of dimensionality. \n",
    "Using a majority vote to assign the class label can be a problem when the nearest neighbors' distance is vary widely. The distance / similarity metric do not consider the relation of attributes, thus we can get wrong classification due to the presence of irrelevant attributes. To address this issue, we can add some weight on the voting. <br> <br>\n",
    "$$ \\text{Weighted-voting} = \\underset{v}{\\arg\\max}\\ \\sum_{(x_i, y_i) \\in D_z} w \\times I(v = y_i) $$\n",
    "<br> Another approach is by doing Backward Elimination\n",
    "<br> <br>\n",
    "3. The choice of the distance measure <br> <br>\n",
    "4. High complexity. Since KNN are lazy learner, means that this algorithm does not build any model explicitly. To classify new object, it needs to compute the distance of the unlabeled object to all the labeled data in training set. So classifying new unlabeled object is relatively expensive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOURCE\n",
    "* http://www.cs.umd.edu/~samir/498/10Algorithms-08.pdf\n",
    "* http://www.cs.upc.edu/~bejar/apren/docum/trans/03d-algind-knn-eng.pdf\n",
    "* http://mi.eng.cam.ac.uk/~ky219/papers/yu-npl02.pdf\n",
    "* http://www.kdnuggets.com/2016/01/implementing-your-own-knn-using-python.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
