{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes theorem is applied with 'naive' assumption of the independence among each features.\n",
    "\n",
    "It is used in classification problem when we know its prior probability ( supervised learning algorithms ).\n",
    "\n",
    "Great for very high dimensional problems because of its strong assumption; and only required a small amount of training data (it doesn't suffer from curse of dimensionality)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem statement: \n",
    "* Given features $x_1, x_2, \\ldots, x_n $  \n",
    "* Predict a label $Y$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given joint distribution on $x_1, \\ldots, x_n $ and $Y$, predict the probability of belonging to a label by using: \n",
    "\n",
    " $ \\underset{Y}{\\arg\\max}\\  P(Y \\ |\\  x_1, \\ldots, x_n ) $\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes Rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"bayes_rule.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every label in $Y$, we will compute each of their probability and choose the highest one as our prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, there are 2 class label, $Y = 5$ or $Y=6$. We would compute probability for each class label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "P(Y = 5 \\ |\\  x_1, \\ldots, x_n ) & = \\frac{P( x_1, \\ldots, x_n \\ |\\ Y = 5) \\ P(Y=5)}{P( x_1, \\ldots, x_n \\ |\\ Y = 5) \\ P(Y=5) + P( x_1, \\ldots, x_n \\ |\\ Y = 6) \\ P(Y=6)} \\\\ \\\\\n",
    "P(Y = 6 \\ |\\  x_1, \\ldots, x_n ) & = \\frac{P( x_1, \\ldots, x_n \\ |\\ Y = 6) \\ P(Y=6)}{P( x_1, \\ldots, x_n \\ |\\ Y = 5) \\ P(Y=5) + P( x_1, \\ldots, x_n \\ |\\ Y = 6) \\ P(Y=6)}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the highest probability between both class.\n",
    "\n",
    "If $P(Y = 5 \\ |\\  x_1, \\ldots, x_n )$ is higher than our prediction would be belong to class label 5 instead of 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the Bayes Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"naive_bayes_intuition.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the top 10 classification algorithms are discriminative (K-NN, CART,\n",
    "C4.5, SVM, AdaBoost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Assumption:   Features are independent given class. So,\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\begin{align}\n",
    "P(X_1, \\ldots, X_n  \\ |\\  Y ) &= P(X_1 \\ |\\  Y ) P(X_2 \\ |\\  Y ) \\ldots P(X_n \\ |\\  Y ) \\\\\n",
    " &= \\prod_{i=1}^{n} P(X_i \\ |\\  Y )\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of learning a joint distribution of all features, we learn $P(X_i\\ |\\ y)$ separately for each feature $X_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given :\n",
    "* Prior $P(Y)$\n",
    "* $n$ conditionally independent features $X$ given the class $Y$\n",
    "* likelihood $P(X_i \\ | \\ Y) $   for each $X_i$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Rule:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \n",
    "\\begin{align}\n",
    "y^* = h_{NB} (x) & = \\underset{Y}{\\arg\\max}\\ P(y) \\ P(x_1, \\ldots, x_n \\ | \\ y) \\\\\n",
    "& = \\underset{Y}{\\arg\\max}\\ P(y)  \\prod_{i=1}^{n} P(X_i \\ |\\  Y )\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential problem : \n",
    "    \n",
    "    Most of the conditional probabilities are 0 because the dimensionality of the data is very high compared to the amount of data. This causes a problem because if even one \n",
    "$$P(X_i \\ | \\ Y)$$\n",
    "\n",
    "    is zero then the whole right side is zero. \n",
    "    In other words, if no training examples from class “spam” have the word “tomato,” we’d never classify a test example containing the word “tomato” as spam!\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to problem:\n",
    " \n",
    "     Laplace Smoothing (or “Bayesian shrinkage estimate”)\n",
    "     \n",
    "$$   P(X_i = x \\ | \\ Y = y ) = \\frac{n_{ik} + 1}{n_{k}+K} \\\\$$\n",
    "\n",
    "$ \\ n_{ik} =  \\# \\text{ of examples with } Y_i = y, X_i = x $ \n",
    "\n",
    "$ \\  n_k = \\# \\text{ of examples with } Y_i = y$\n",
    "\n",
    "$ \\  k = \\# \\text{ of possible values of } X $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Naive Bayes is not necessarily the best algorithm, but is a good first thing to try, and performs surprisingly well given its simplicity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
