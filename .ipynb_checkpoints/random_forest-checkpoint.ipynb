{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Tree Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification tree constructs prediction models from data. \n",
    "\n",
    "The models are obtained by recursively partitioning\n",
    "the data space and fitting a simple prediction model within each partition. The partition is done by asking a series of question related to the attributes of the data. Each time we received an answer, a follow up questions is asked until finally we get the conclusion about the class label of the data.\n",
    "The series of questions and answers can be organized in the form of a decision tree. \n",
    "a hierarchical structure consist of nodes and directed edges. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In random forest algorithm, instead of using one trees, we grows a lot of trees. We put input data to each of the trees in the forest. Each tree would gives a classification, assume this like a 'votes' for that class. Forest will choose the classification with the most number votes among all the trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If the number of cases in the training set is $N$, draw $N$ bootstrap sample or in other word sample $N$ cases at random (with replacement) from the original data. This sample will be our training set for the tree.\n",
    "<br> <br>\n",
    "2. For each sample, grow a tree with random variables. If there are $M$ input variables, a number $m << M$ is specified so for each node, $m$ variables are selected in random among all the $M$ variables and the best split of these $m$ is used to split the node.\n",
    "<br> <br>\n",
    "3. Predict new data by aggregating the predictions of the $N$ trees, by choosing the majority votes for classification among all the trees. \n",
    "<br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for each tree, the either the training set or variable would be randomly picked. \n",
    "<br>Each tree should be grown the the largest extent possible or we can say there is no pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-bag (OOB) error estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An error rate can be estimated from the training data itself ( no need for cross validation or test set ) as follows:\n",
    "<br> <br>\n",
    "Each tree is constructed using different bootstrap sample from the original data. About one-third or 36% of the data is left out of the bootstrap sample which called \"out-of-bag\" or OOB. Because this OOB data is not is our bootstrap sample, it is not used in the construction of the k-th tree. \n",
    "<br> <br>\n",
    "Use this OOB data from k-th tree and using that tree to get a classification (prediction). \n",
    "<br> <br>\n",
    "At the end of the run, calculate the error rate. Take the class that got most votes for every OOB cases (aggregate the OOB prediction). \n",
    "<br>\n",
    "The proportion of times that class is not equal to true class of n averaged over all cases would be the error estimate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance of a variable may be due to its interaction with other variables. Random forest algorithm estimates the importance of a variable by looking at how much the prediction error increases when OOB data for that variable is permuted while others are left unchanged. Calculation to score the importance would be carried out tree by tree as the random forest constructed. \n",
    "\n",
    "If the number of variables is very large, forests can be run once with all the variables, then run again using only the most important variables from the first run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximities\n",
    "\n",
    "In the NxN proximity matrix, the (i,j) element is the fraction of trees in which element i and j fall in the same terminal node. The larger number means they are more simmilar. THe idea is that similar observartion should be in the same terminal nodes more often than dissimilar ones. Using this matrix we can indentify the structure of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criterion\n",
    "\n",
    "* Gini\n",
    "\n",
    "* Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: <br>\n",
    "http://www.bios.unc.edu/~dzeng/BIOS740/randomforest.pdf <br>\n",
    "https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
