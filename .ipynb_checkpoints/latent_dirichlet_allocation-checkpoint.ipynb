{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is an ** Unsupervised Learning **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA can model heterogeneity while classical mixtures cannot. It means each document is not limited to exhibit one topic, but multiple topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is a probabilistic modelling, so the process are:\n",
    "\n",
    "\n",
    "1. Data are assumed to be observed from a generative probabilistic process that includes hidden variables.\n",
    "<br>\n",
    "   --> In text, the hidden variables are the thematic structure.\n",
    "<br> <br>\n",
    "2. Infer the hidden structure using posterior inference\n",
    "<br>\n",
    "   --> What are the topics that describe this collection?\n",
    "<br> <br>\n",
    "3. Situate new data into the estimated model.\n",
    "<br>\n",
    "   --> How does a new document fit into the topic structure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative model is illustrated in the picture below\n",
    "* Each topic is a distribution over words\n",
    "* Each document is a mixture of corpus-wide topics\n",
    "* Each word is drawn from one of those topics\n",
    "<img src = 'image/generative_model_lda.png'>\n",
    "\n",
    "\n",
    "\n",
    "##### In reality, we only observe the documents. The other structure are hidden variables\n",
    "##### Our goal is to infer the hidden variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we draw lda into graphical model it looks like this:\n",
    "<img src = 'image/lda_graphical_model.png'>\n",
    "\n",
    "*the shaded area is the observed variable* <br>\n",
    "*plate means vectors * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the joint distribution of the observed in hidden random variables, which is also describe the LDA model\n",
    "<img src = 'image\\lda_model_equation_2.png'>\n",
    "\n",
    "So, we got 2 dirichlet distribution here ( beta & theta -> dirichlet distribution )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling Definition:\n",
    "* help you automatically discover patterns in a corpus [1]\n",
    "* algorithms that uncover the **hidden thematic structure** in document collections. These algorithms help us develop new ways to search, browse and summarize large archives of texts [2]\n",
    "* provide a simple way to analyze large volumes of **unlabeled** text. A \"topic\" consists of a **cluster of words** that frequently occur together [3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modelling is the soft/fuzzy clustering: each document can belong to different degree and different clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/type_of_clustering.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flow of topic modeling is shown on the diagram above. Where we input the collection of text documents as dataset and get topics as output. While the black box is the algorithm we use (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagram of topic modeling\n",
    "<img src=\"image/topic_modeling_diagram2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm:\n",
    "\n",
    "1. Initialize parameters\n",
    "   <br>\n",
    "   such as: <br>\n",
    "   * K ( # of topics ) must be defined by us before\n",
    "   * number of iterations\n",
    "   <br> <br>\n",
    "2. Initialize topics randomly\n",
    "   <br> \n",
    "   Each word in each document is gonna assign topics randomly\n",
    "   <br>\n",
    "   <img src=\"image/document_topic.png\" width = \"30%\" height = \"30%\">\n",
    "   <br>\n",
    "3. Iterate\n",
    "   <br>\n",
    "   Resample topic for word (given all other words and their current topic assignment):\n",
    "   Remove the assigned topic in each word with a \"better\" topic by answering these 2 questions: <br>\n",
    "   * Which topics occur in this document?\n",
    "   * Which topics like the word X?\n",
    "   <img src=\"image/document_topic_change.png\" width = \"35%\" height = \"35%\">\n",
    "   <br> <br>\n",
    "4. Get result\n",
    "   <br>\n",
    "   We get final **psi** and **phi**  ( frequency of words and distribution of topics )\n",
    "   <br> <br>\n",
    "5. Evaluate model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/topic_modeling_diagram3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation \n",
    "\n",
    "1. Human-in-the-loop\n",
    "   * Word Intrusion [5]\n",
    "   <br>\n",
    "     On each topic take words randomly and substitute ones with another (intruder words). See whether human can detect which on it was (the intruder words). If so, then topic is good.\n",
    "   <br> <br>      \n",
    "   * Topic Intrusion \n",
    "   <br>\n",
    "     Add topic that doesn't belong to a document to your document and ask user to find which topic is not represented \n",
    "          \n",
    "     <img src=\"image/human_in_the_loop.png\" width = \"50%\" height = \"50%\">          \n",
    "     <br>\n",
    "2. Metrics [4]\n",
    "    \n",
    "   * Cosine similarity\n",
    "       Split document into 2 parts, check :\n",
    "       - intradistance (the topics must be distributed similarly)\n",
    "       - interdistance (the first half similar to topics and second half of different document are mostly dissimilar)\n",
    "       <br> <br>\n",
    "   \n",
    "   * Size (# of tokens assigned)\n",
    "   * Within-doc rank\n",
    "   * Similarity to corpus-wide distribution\n",
    "   * Locally-frequent words\n",
    "   * Co-doc Coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python libraries\n",
    "\n",
    "Gensim: https://radimrehurek.com/gensim/\n",
    "<br>\n",
    "Graphlab: https://dato.com/products/create/docs/generated/graphlab.topic_model.create.html\n",
    "<br>\n",
    "lda: http://pythonhosted.org//lda/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "<img src=\"image/pipeline.png\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our words in the text document have to be preprocessed before. There are two kinds of **PREPROCESSING**:\n",
    "1. Tokenization\n",
    "   The way we divide the words in the documents\n",
    "   * simple : just word by word\n",
    "   * collocations : combine 2/3 words together because it's more meaningful\n",
    "   * entities : might be only adjective, or only something\n",
    "   * combination : mix\n",
    "   * lemmatization : grouping words with the same variant\n",
    "   <br> <br>\n",
    "2. Stopwords\n",
    "   We may want to omit stopwods from\n",
    "   * language generic : a, above, across, after, all, the ...\n",
    "   * domain specific : material, size, advance ..\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the preprocess data would be convert into **VECTOR SPACE**\n",
    "<img src = \"image/vector_space.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a deeper insight, we can **VISUALIZE** it with a library called LDAVis \n",
    "<img src = \"image/LDAVis.png\">\n",
    "https://github.com/bmabey/pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOURCES:\n",
    "\n",
    "[1] https://www.cs.jhu.edu/~jason/465/PowerPoint/lect-topicmodels-mpaul.pdf, \n",
    "<br>\n",
    "[2] http://www.cs.princeton.edu/~blei/topicmodeling.html, \n",
    "<br>\n",
    "[3] http://mallet.cs.umass.edu/topics.php\n",
    "<br>\n",
    "[4] http://mimno.infosci.cornell.edu/slides/details.pdf\n",
    "<br>\n",
    "[5] http://www.umiacs.umd.edu/~jbg/docs/nips2009-rtl.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
