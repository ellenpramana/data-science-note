{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is about **NATURAL LANGUAGE PROCESSING** (NLP) based on Peter Norvig notebook <br>\n",
    "\n",
    "http://nbviewer.jupyter.org/url/norvig.com/ipython/How%20to%20Do%20Things%20with%20Words.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Boring preliminaries\n",
    "%pylab inline\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data: Text and Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can do things with words, we need some words. First we need some text, possibly from a file. Then we can break the text into words. I happen to have a big text called big.txt. We can read it, and see how big it is (in characters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWL06.txt         count_1w.txt      count_big.txt     ngrams.py\r\n",
      "ch14-old.pdf      count_2l.txt      enable1.txt       sowpods.txt\r\n",
      "ch14.pdf          count_2w.txt      index.html        spell-errors.txt\r\n",
      "count_1edit.txt   count_3l.txt      ngrams-test.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "564779"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('nlp_data/big.txt').read()\n",
    "len(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, five hundreds thousand characters.\n",
    "\n",
    "Now let's break the text up into words (or more formal-sounding, tokens). For now we'll ignore all the punctuation and numbers, and anything that is not a letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokens(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    return re.findall('[a-z]+', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'test', 'this', 'is']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens('This is: A test, 1, 2, 3, this is.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106274"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORDS = tokens(file)\n",
    "len(WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, a hundred words. Here are the first 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'adventures', 'of', 'sherlock', 'holmes']\n"
     ]
    }
   ],
   "source": [
    "print(WORDS[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Models: Bag of Words\n",
    "The list WORDS is a list of the words in the TEXT, but it can also serve as a generative model of text. We know that language is very complicated, but we can create a simplified model of language that captures part of the complexity. \n",
    "\n",
    "In the bag of words model, we ignore the order of words, but maintain their frequency. Think of it this way: take all the words from the text, and throw them into a bag. Shake the bag, and then generating a sentence consists of pulling words out of the bag one at a time. Chances are it won't be grammatical or sensible, but it will have words in roughly the right proportions. \n",
    "\n",
    "Here's a function to sample an n word sentence from a bag of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample(bag, n=10):\n",
    "    \"Sample a random n-word sentence from the model described by the bag of words.\"\n",
    "    return ' '.join(random.choice(bag) for _ in range(n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'follow within of darker it been he from much small'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another representation for a bag of words is a Counter, which is a dictionary of {'word': count} pairs. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a': 2, 'is': 2, 'it': 1, 'test': 2, 'this': 1})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tokens('Is this a test? It is a test!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 5654), ('i', 3038), ('and', 3024), ('to', 2752), ('of', 2674), ('a', 2647), ('in', 1771), ('that', 1752), ('it', 1735), ('you', 1504)]\n"
     ]
    }
   ],
   "source": [
    "COUNTS = Counter(WORDS)\n",
    "print(COUNTS.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5654 the\n",
      "3 rare\n",
      "3024 and\n",
      "0 neverbeforeseen\n",
      "46 words\n"
     ]
    }
   ],
   "source": [
    "for w in tokens('the rare and neverbeforeseen words'):\n",
    "    print(COUNTS[w], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 1935, linguist George Zipf noted that in any big text, the nth most frequent word appears with a frequency of about 1/n of the most frequent word. He get's credit for Zipf's Law, even though Felix Auerbach made the same observation in 1913. If we plot the frequency of words, most common first, on a log-log plot, they should come out as a straight line if Zipf's Law holds. Here we see that it is a fairly close fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEOCAYAAACAfcAXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd4FNX6wPHvm4RepCmCIEV6E0WQohCKIKCAYkMBARVE\nBa/t/uhJiBT1ggXERlFEFLHABSxcSsCCikqRXqQpiNJLKCE5vz/OBJaQskm27/t5nn2yU3bOu5PZ\nfffMmTlHjDEopZQKPxH+DkAppZR/aAJQSqkwpQlAKaXClCYApZQKU5oAlFIqTGkCUEqpMKUJIAyI\nyBUislxEjorIS14qY4eItPLGtr1JRJ4XkX9EZK+/YwlWIhIjIu/7qCy/HGeu71FEyovIMRERX8fh\naUGdAERkp4gkOv+M487fK/0dVwDqC/xtjLnMGPNcbjcmItNEZKQH4vIqEUkRkcqZLC8PPA3UMMaU\n9V1kued8IU33dxwu/H5DkYhEi8gSETkiIr9nsE5jEfkuh0UYAGPMHmNMURMCN1EFdQLA/kM6Ov+M\nIs7fv9KuJCKRfogtkFQANvg7CD/I6gNaAThgjDmY3kI9bi4V4L96TwJTgGczWacjsMA34QQBY0zQ\nPoAdQKt05lcAUoA+wC4gwZnfGPgOOAysAlq4vKYikAAcBb4GJgDvO8taAHsyKhsQYBCwDfgH+Ago\nliaWnk4sfwNDXLYTAQxxXnsMWAlcBUwE/pOmzLnAkxnsi6bAT857+xFo4syfBpwFzjjbT29/TXPK\nm++sswKolEE5jzjbO+2sO9dlfzwDrHFi+BDIm8E2HgS+BcY7624DmjjzdwN/AT1d1i8KTHf23Q5g\nqMuya5z/2xFn+YfO/GXOfj/hxHl3mhhaA4nAOWf51GA5boB2zv/zDHAcWJXOPu4F/Ndleiswy2V6\nN1Avs2PHWbYUeN75f50EKmfwnqdn8L8uBsxz4j/oPL8qzfZHOts/BnwFlHBZ3gPY6eyfIWTwmU/n\nf/t7Bst+Aa51nqcA/YAtwCFgYibbjEl9jy7/mwg330OGx4+/H34PIFfBZ50A3gUKAPmAssABoJ3L\nQXIAKOlMfw+8BOQBbnb+kan/8BbA7ozKBp50Xl/Gef0bwMw0sbwF5AXqYb88qzvLn8N+aVZxpusC\nxYGGwB8u5ZXEfpmVSuf9FncO4PuxCeU+Z7q4s3waMDKT/TjN+YA1cF4/IzX+TNYfmc7++AEojf3Q\nbwD6ZvD6B7FJpCf2SzAe+yU3wdl/tzj7v6Cz/nTgc6Cgsz83A72dZTOBwc7zvEBTl3JSyCCRpfd/\nDbLj5vwXUgbvrRJwyHleBvslutuZrgwcdJ6XyOLYWeq8toazPCqz95xOHCWAO5x9WQiYBXzusnwp\nNjld46yzFBjtLKuFTXDNnLLGOcdNjhIAcCUuCdnZv/8FigDlsUmqbQbbTJsAkrk4AWT0Hq7K7Pjx\n98PvAeQqePthOuYcsIeAz9L8gyq4rPtv4L00r/8K+wujvHNgFXBZ9gHuf5A3AC1dlpVxthfhEksZ\nl+U/Avc4zzcBt2Xw/tYDrZ3njwPzM1ivO/BDmnnf4/yKxr0E8LbLdHtgQxbrp5cAurlMvwBMyuD1\nDwKbXabrOPuolMu8A9gvvQjsL93qLsv6Akuc5+8Bb+Lyq9JlvRSgcibvI70EECzHTaYJwFlnF1Af\nuBebSH4AqmFrB3PcPHaWArEuyzJ9z258ZuvjJB+X7bvWiPsDXzjPh+PyQwT7A+AMOU8AfYB30hwf\nrrWdWcC/M9hmVgkgo/eQ4fHjzv7y9iPY2wAAOhtjSjiPO9Ms+8PleQXgHhE55DwOY39ZlMH+yjts\njDnlsv6ubMRQAfg8ddvYD3YS9tdwqv0uzxOBws7z8kC6DVbYX77dnefdgYyutCibTry7sL8+3OXa\ndnI+PhEZ7NLAPimLbWT0HrNa9xSAMeZAmnmFgVLYX527XZa5vrd/Y78wfxKR30SkdxYxuiMYjht3\nLANaAs2xp2wSgGhsYlrmrOPOsbPH5Xm23rOIFBCRt5wLNo445RZL05aQ7rHnlHW+bGNMIvY0Uk51\nAL5IMy83+9dVRu8hs+PH70IhAWTWKGVcnu/BZvDUZFHc2IbjF4F9QHERKeCy/tUuz09if33YAm3j\n4OUuy3cD7dNsu5AxZp8b8e/BVh3TMwPoLCL1sFXwORmstxd7XtbV1cCfbpSfKWPMGHOhgf2x1Nm5\n3W42HMB+KVZwmVcB570ZY/YbY/oaY64CHgUmZXblj5uC4bhx53+wHPuFfxP2i3c59su/ORcSgDvH\njmtZWb3ntJ4BqgINjTHFnLIh88+ta1nlUydEpCD2VGi2iUgU9r3/Lyevz4XMjh+/C4UEkJG0B9gM\n4HYRaSsiESKSX0RaiEhZY8xu4GcgTkTyiMhNwO0ur90C5BeR9s6BNAx7XjbVW8BoEbkaQEQuF5FO\nmcTiajIQLyJVnNfWFZHiAMaYP5243gc+NcacyWAbXwBVReQ+EYkUkXuBmthGXW/Yjz2P7Enp7iNj\nTArwMTBKRAqLSAXgKZzakIjcJSKpv1aPYKv1Kc70XzmIM1iOm/1AxSyuykmtARQwxuwFvgFuxX6J\nrnLWyejYmZfeBt14z2kVwdbmjolICSA2k3XT+gS4TUSaikgebENrhu9XrHzYfRwhIvmc14FNgmuM\nMSeyUX5m3L0aKsPjx0Nx5IpPEoCIFBSRlSLSwcObzuxX0EXLjDF/AJ2xVxL8g622PsuFffAAtrX+\nIPbc43surz0GPIa9xOwPbMOU62mCV7FX6CwUkaPYc6iNMonTdXo89gsu9bWTsQ2Qqd7DniPP8Jpv\nY8wh4Dbn/Rxw/nZ05qdX/iWbyGJ5WlOA2k6V9rMcbiOrGFynB2Kr1b9jf8XOMMZMc5Y1BH4UkWPY\nGtJAY8xOZ1ksMN2J866cxBHAx81s7JfQQRH5Od03YsxWp8zlzvRxYDvwrXFORmdy7BzOIAawDcbp\nvud0vIKtBR1w3l/aUzAZHjfGmA3Ytq8PsTWVg1y8/9Jqjk0287E1h0TsVUpgL//MquzsHMMmg+cX\nr5TF8SMib7hxatVrxDkOvFuISBz2QNxgjEn7TwhIIhIDXGOM6ennOG7GXlZY0Z9xKPcEynGjLiYi\n64GuxphN/o4lkGS7BiAiU0Rkv4isTTP/VhHZJCJbROT/XOa3wTZu/YP71SYFONXXJ4F3/B2LUsHK\n+Ry9p1/+l8rJKaBp2BtRzhORCOyNRO2A2kA3EanhLI4GbsRWGx/OcaRhxtl/h7FXhLzq53CUClrG\nmKRAaXQNNDk6BeQ0xM0zxtRzphsDMcaY9s70IMAYY15weU1P7G33QXEKSCmlQl2Uh7ZzFRdfK/wH\nFzdmYYzJsBFTRHx5WaFSSoUMY0yOT60HzGWg3r7jLSYmxuuvzWq9zJZntCzt/PTWc52Xm/fpi/2Z\nndd5en+6M8+d/R0o+9JX+zM788NlfwbCZ92Y3P9u9lQC+JOLbwYpRzZvQoqNjSUhIcFD4VwqOjra\n66/Nar3Mlme0LO389NbLzXvLqZyWmZ3XeXp/ujMvmPZldl+b0/2Znfnhsj/9/VlPSEggNjY20xjc\nkdM2gIrYNoC6znQktoOu1ti7937C9guz0c3tGU9kM2UTqScODGXp/vQs3Z+eJSIYX54CEpGZ2Bs6\nqonIbhHpbYxJBgYAC7EdmH3k7pe/8ix//OIKZbo/PUv3Z2DxyY1gWQYhYmJiYoiOjtYDRCmlspCQ\nkEBCQgJxcXG5qgEETAIIhDiUUiqY+PwUkLd4uxFYKaVChV8bgT1NawBKKZV9IVMDUEop5VsBkwD0\nFJBSSrlHTwEppVSY01NASimlckQTgFJKhamASQDaBqCUUu7RNgCllApz2gaglFIqRzQBKKVUmAqY\nBKBtAEop5R5tA1BKqTCnbQBKKaVyRBOAUkqFqSh/B5Bqxw4Qcf8B2Vu/QAGI0HSnlFLnBUwbQIUK\nBmNw6wHuref6SEmBcuWgYkX7qFDhwvOKFaFsWYiM9NsuUEqpbMttG0DA1ABa3/EQD3R6gFYtW3ll\n+6dPw+7dsHPnhceXX8KuXfb5gQNw1VWXJocKFaBkSSha1D6KFIGogNlrSqlwlDokZG4FTA2gzqQ6\n/HPyHzpV70SXGl1oVakV+aPy+yyG06dhz54LCcH1cfgwHD8Ox47Zv/nyXZwQUp8XKgR589rlefNe\n+qhVC+64w2dvSSkV4nJbAwiYBGCMYfuh7czdPJc5m+awdv9a2l7Tli41utChageK5S/m7zABezop\nMdEmg9SEkPr85Ek4e/bix5kzF55/8AGMGQP33+/vd6GUCgUhlQBc/X3yb+Zvmc+cTXNI2JnAjeVu\npEv1LnSu0ZlyRcv5KdLc+e03aN0aPv8cmjXzdzRKqWAXsgnA1cmzJ1m4fSFzNs9h/pb5VC5emS7V\nu9ClRhdqXV4LkRy/f5/76ivo3Ru++w4qV/Z3NEqpYBYWCcBVUnIS3+7+ljmb5jBn8xzyRualc/XO\ndKnRhSblmhAZEfiX8rz+OkycCCtWQLHAOLOllApCYZcAXBljWP3X6vPtBvtO7OP2arfTpUYXWldq\nTYE8BbwQrWc8+SSsX2+vRMqTx9/RKKWCUcgkgJiYGKKjo4mOjs7xdnYc3nE+Gaz6axVtKrehS/Uu\ndKzWkRIFSnguYA9ITobOne39B2+9deHmNqWUykrqZaBxcXGhkQA8HceBxAPM3zKfuZvnsmTHEpqW\nb0qva3vRuUZnn15empnjx+Gmm6BnT3jmGX9Ho5QKNiFTA/BmHIlJiXy+8XOmrZ7G6r9Wc2/te+l9\nXW8alGng9wbkPXugSRN7VVCRIlCw4IVHgQIXTxcsaGsMDRr4NWSlVIDQBJBNu47sYvqa6by75l0K\n5ilI7/q96V6vO1cUusIn5afn999tg3Bion2cOnXhedrptWttreGVV+Dyy/0WslIqAGgCyKEUk8I3\nu75h2uppzN08lxYVWtCrfi86Vu1InsjAbZVNTIQRI2DGDBg/Hrp10/YDpcKVJgAPOH7mOLM3zGba\n6mlsObiFB+o+QO/6valbuq7fYsrKypXw0EO2g7tbboESJeyjdGm47jq9skipcKAJwMO2HtzKu6vf\nZfra6ZQuVJre9XvzQL0HAqYrCldnz8LkybB1Kxw8CIcOwR9/2P6LWrWCW2+Fe+7Rew2UClWaALwk\nOSWZxTsWM3XVVL7e/jV31byLxxo+xnVlrvN3aFn6+2/43/9g9mzYuxeWLbMNykqp0KIJwAf2n9jP\nlFVTePPnN7mq6FX0v6E/99S+J2AuJ82IMfDAA/b5Bx9oW4FSoSbgE4CI1ACeBEoCS4wxb6azTkAn\ngFTJKcks2LqASSsn8cu+X+hdvzeP3vAolYsHbqc+p05Bixb2prOhQ/0djVLKkwI+AZwvyF5w/54x\npmc6y4IiAbjadmgbb/78Ju+ufpdGVzXisYaP0b5K+4Dsi2jfPrjxRnj8cWjTBmrXhvyBXXlRSrkh\ntwkg26PkisgUEdkvImvTzL9VRDaJyBYR+b80y24H5gNf5DTQQFOlRBX+0/Y/7HlqD/fWvpeRy0ZS\nZUIVXvnhFY6dOebv8C5SpgwsWADr1tmeSIsXt5ePnjjh78iUUv6U7RqAiNwEnACmG2PqOfMigC1A\na2AvsBK4zxizKc1r5xtjbktnm0FXA0jPD3/8wMs/vMyi3xfx4LUPMvDGgVQsVtHfYV0iMdHWBn75\nBebOhUqV/B2RUionfF4DMMZ8CxxOM7sRsNUYs8sYkwR8BHR2AmwhIq+KyJvAgpwGGgwal2vMrLtm\nsarfKiIlkgZvN+Du2XezYs8Kf4d2kYIFYepUePhh2w3FnDn+jkgp5Q85agMQkQrAPJcaQFegnTGm\nrzPdHWhkjBno5vZMTEzM+enc9goaKI6fOc601dN49cdXubzg5TzV+Cm61upKVETgjCr/7bf2tFDD\nhvDaa1CqlL8jUkplJO1g8H7pDdQbCSAUTgFlJDklmXlb5jF+xXh2Hd3FgEYDePj6hwPm5rLERBg2\nDD780A5Wc+ed/o5IKeUOn58CysCfwNUu0+WceW6LjY29KLOFksiISLrU6MLy3sv59J5PWfXXKiq/\nWpmBXw5k+6Ht/g6PggVtv0KffgqDB8Pdd9u2gX/+8XdkSqn0JCQkEBsbm+vt5LQGUBFbA6jrTEcC\nm7GNwPuAn4BuxpiNbm4vpGsA6fnj2B+8/tPrTF41mZuuvolnmzxLs6v9P1L8qVN2uMpFi+CHH+yY\nBVFRcO+9MGGCdiuhVCDx+X0AIjITiMbe2LUfiDHGTBOR9sAr2FrFFGPM2Gxs0yMjggWjk2dP8t6a\n9xi3Yhxli5RlULNBdKjawe/jFIC9kzglBU6ehEGD4IsvoEMHyJcP+vSBuoHbV55SIU1HBAsx51LO\n8cmGTxj77VhSTAr/1+z/uLfOvQHVYLxsmR3H+MABWxvo2xeef167mFDKX4LmTuBMg9AEcJ4xhq+3\nf82Yb8ew++hunmv6HL3r9w64Ae7/+gtuvx3uu0+Hs1TKXwKlETjXQrkRODtEhFur3MqyXsv44M4P\n+Hr711R6tRKjvxnNkdNH/B3eeVdeCZ98Ai+9ZBuMk5L8HZFS4cOvjcCepjWAzK3/ez0vfv8i87fM\n56HrHuJfjf9F2SJl/R0WAIsXw5NP2jEISpe2jcTFi9shK+vU8Xd0SoU2PQUURnYd2cX4FeN5f+37\n3FXrLp5r+hxVS1b1d1iAvVpo/344csSOQ7B7t72vQCnlPSGTAML1KqCcOJB4gAk/TmDSz5NoWbEl\nQ24eQv0r6/s7rPOOHLH9C23YYDuiU0p5ll4FpDhx9gRv/fwW41aM44ayNzC8+XAaXtXQ32EB0L8/\nFCkCL7ygVwkp5S0hUwMIhDiC1amkU0z+dTIvfv8ida6ow/Dmw2lavqlfY/r9d+jUyd4zMHiwHZBG\nB6pXyrP0KiBFgTwFGHDjALYN2EaX6l24/9P7aTO9Dct2LvNbTJUrw9q1MGKE7WaiUCGoVg0GDrQD\n2Culck6vAlIZSkpO4v217zP6m9GULVKWES1G0LpSa7/eXZyUZNsEpkyBWbOgVi07XnGfPhARMD9D\nlAouegpIZehcyjk+WvcRo74ZRbH8xRjefDjtq7T3ezcTW7bAtm0QH2+nn3kG6tWDa66ByMAbUVOp\ngKUJQGUpOSWZTzd+SvzyePJF5mN48+F0qt7J74kgJQXee88OSLN6NdSoYYeujAqc3i+UCmiaAJTb\nUkwKczfNJX55PMkmmWE3D6Nrra5EiP/PwZw7Bx072nsJ6ta1l5FWqgSXXQZt20Lhwv6OUKnAEzIJ\nQO8D8B1jDAu2LiB+eTwnzp5gePPh3FP7Hr8ngtOn4aefYMcOexXRjh22z6FffrFtBjffDEOH2gZl\npcKZ3gegcs0Yw8LtC4ldFsuxM8eIaRHDXbXu8nsiSGvvXttuMHUqfPMNvPWWrRUoFe5CpgYQCHGE\nK2MMX237ipiEGE6dO0Vsi1juqHlHwCUCgK++gkcfhehoePFFuOIKf0eklP9oAlAeY4zhi61fEJMQ\nQ1JKErEtYulSo4vfG4vTOnHC3l8wdSo0agTdutmB7ZUKN5oAlMcZY5i3ZR6xCbEAxEbHcnu12wMu\nERw7Bv/7H/TrB2+/rYPZq/ATMglAG4EDjzGGuZvnEpsQS1REFLHRsXSs2jHgEsGiRTYJ3H03PPww\nVKni74iU8i5tBFY+k2JS+Hzj58Qui6VAVAHiouO4tcqtAZUI9uyxw1N+9pk9JdShA9xyi95YpkJb\nyNQAAiEOlbkUk8KnGz4ldlksRfIWIS46jrbXtA2oRLB+vb2xbPZsKFoU3nzTXkKqVCjSBKB8Ljkl\nmdkbZhO3LI4SBUoQFx3n976G0kpOtl1NfPONHbVMqVCkCUD5TXJKMrPWzyJuWRxXFLqC+JbxRFeM\n9ndY5yUlQe3a8Nxz8NBD2umcCj2aAJTfnUs5x8zfZhK3LI7KxSsT3zKexuUa+zsswPZA2qED3HCD\nHcReqVCiCUAFjKTkJKatnkb88njqX1mf+JbxATFU5YkTUKqUvW/g/vv9HY1SnhMyA8Ko4JcnMg99\nG/Rl64Ct3FL5Ftp/0J57Zt/Dxn82+jWuwoXtAPUDBsDo0bafIaVUACUAHREsdOSPys/AGweybcA2\nGpRpQIt3W9Dz8578fth/37x33AHffQerVkGzZqAVThXMdEQwFTSOnj7KKz+8woSfJtC1ZleGtxhO\nuaLl/BKLMVC/vh2feORI2z6gVLDSNgAVNA4mHuSl71/inV/foUe9Hgy+aTClC5f2eRzHjkFCgr1h\n7JFH4K67oGlTvUpIBR9tA1BBo2TBkoxtM5b1j63HGEOtSbUYvGgwh04d8mkcRYtCp06wbJkdcOaR\nR6BnT9i+3adhKOV3mgCUz11Z+Epebf8qq/ut5tCpQ1SbUI2Ry0Zy7Mwxn8Zxww0QF2cHnDl9Gho3\ntt1JpKT4NAyl/EZPASm/235oO3HL4vhq21c82/RZHm/4OIXy+n7Yr23boE0buO46e/NYkyYQQDc3\nK3UJPQWkgt41Ja5h+h3TSeiVwM97f6bqhKpM+HECZ86d8WkcVarA2rW2JnDnnbaB+JBvz04p5VNa\nA1ABZ9W+VYxIGMGav9YwvPlwetXvRZ7IPD6N4exZ6N8f1q2DH37QmoAKTHoVkApZK/asYPjS4ew8\nspPY6Fi61elGZITv+ndOSbEjjjVvbhuKa9b0WdFKuSXgTwGJSGcReVtEPhSRW7xdngodTco3YVHP\nRbxz+ztMWjmJ+m/VZ+6mufjqx0JEBHz0ERw9CjfeaIehPHzYJ0Ur5RM+qwGISDHgJWPMI+ks0xqA\nypQxhgVbFzB0yVAKRBVgdOvRtKrUymfl//YbjB0LS5bAxx/DzTf7rGilMuTzU0AiMgW4DdhvjKnn\nMv9W4BVsrWKKMeaFNK/7DzDDGLM6nW1qAlBuSTEpzFo3ixEJI6hYrCKjWo2i0VWNfFb+jBnQo4ft\nUyg62l4ppJS/+OMU0DSgXZogIoCJzvzaQDcRqeGyfCzwRXpf/kplR4RE0K1uNzY8toG7a93NnbPu\n5I5Zd7D+7/U+Kb97d/jiC9i9214p9MYbPilWKa/I0SkgEakAzEutAYhIYyDGGNPemR4EGGPMCyIy\nAOgJrARWG2PeTmd7WgNQOXIq6RSTVk7ixe9fpN017YiLjqNS8Uo+KfuXX6B9e6hYEQYOhM6doUgR\nnxStFJD7GkCUh+K4CtjjMv0H0AjAGDMBmJDVBlx7touOjiY6OtpDoalQViBPAZ5p+gyPNHiE8SvG\n0/CdhtxX5z6G3jyUMkXKeLXsBg1g71547z14/XU79OTkyToQvfKehIQEj/aa7KkaQFegnTGmrzPd\nHWhkjBno5va0BqA84kDiAcZ8M4Z317xL3+v78lyz5yhRoITXyz14EFq2tDWA/v2hUiV7Q5kmA+VN\ngXIZ6J/A1S7T5Zx5btPxAJQnlCpYinHtxrHm0TUcPHWQahOqMWr5KE6cPeHVckuWhNWrbQ+j779v\n/z7xhB2NTClP8+t4ACJSEVsDqOtMRwKbgdbAPuAnoJsxxq2hoLQGoLxl68GtjEgYQcLOBAbfNJh+\nDfqRLyqf18vdsgXuvhs2boQaNeyVQ08/rTUC5Vk+rwGIyEzge6CaiOwWkd7GmGRgALAQWA985O6X\nfyqtAShvqFqyKh92/ZAvH/iShdsXUn1idd5d/S7nUs55tdxq1WDNGvjzT3vJ6OuvQ4sWtlsJpXJL\nRwRTKge+2/0dQ5YM4e+TfxPfMp6uNbsiPujo5/BheO01GDPGJofnnrOXlGofQyo3QqYvoJiYGL36\nR/mEMYaF2xcyZMkQBGFUq1G0vaatzxLBf/8LgwZBiRLw7LPQu7fXi1UhJvVqoLi4uNBIAIEQhwov\nKSaFzzZ+xrAlw7iy8JWMbj2apuWb+qTs06dh5kz4979tZ3OvvQbl/DNMsgpigXIVkFJBJ0IiuKvW\nXax7bB0PXvsg3T7txu0f3s7a/Wu9Xnb+/NCnD2zaZKfLl4c5c7xerFIXCZgEoI3Ayl+iIqLofV1v\ntjyxhVsq30K7Ge24/9P72Xpwq9fLLlUKPvsMPvkE7rjDXjp6xrfj4KggpI3ASnnJibMnePWHV3n5\nh5fpWrMrw1sMp1xR75+f2bbNdi1x3XW2x1GlsqKngJTysMJ5CzO0+VC2DNhC8QLFufbNa3l24bMc\nSDzg1XKrVLGXiX79NXTqBCdPerU4pTQBKJWREgVKMLbNWNb1X0diUiI1JtYgflm8V+8qLlkSduyA\nP/6AwoXtIPXz53utOBXmAiYBaBuAClRlipRhUsdJ/Pjwj2w6uMnrg9aXKGF7Gt2yxZ4O6tEDnnoK\nkpK8UpwKQtoGoJSfrPlrDUOXDGX9P+sZGT2S++ve77Wxio2xVwc99ZRtHH76aejZE0qX9kpxKsiE\nzI1ggRCHUtnxza5vGLx4MEfPHGVUq1HcXu12r91MdvYsvPSS7XJ66VLbTtC2rVeKUkFEE4BSfpQ6\nVvHgxYMpkrcIY9uMpXmF5l4tMzYW4uKgVy8YPx6KF/dqcSqAhcxVQNoGoIKRiHBbtdtY3W81jzV8\njAfnPEiHDzqw+i/vjX4aGwvLltl7B8qXhy+/tKeKVPjQNgClAtDZ5LO8/cvbjPpmFC0rtmRky5FU\nKVHFK2WdOQOPPGLHH+jTx45Gpp3LhZeQqQEoFQryRubliUZPsHXAVmpfXpvGkxvz2ILH2Hd8n8fL\nypcPpk+HJUtg6lQ7ItnSpR4vRoUwTQBKeUHqzWSbn9hMoTyFqPNGHQYvGszhU4c9XlbLlnbcgUaN\noFUrePVVPSWk3KOngJTygT1H9zBy2UjmbJ7Ds02eZcCNAyiYp6DHy0lIsKeDIiJg8GDo0sXeXKZC\nk14FpFQQ2XRgE8OXDuf7Pd8zovkI+lzXhzyReTxaxrFj9jLR0aNhzx5YuBCuv96jRagAETJtAHoV\nkAoHNUrVYPbds5l731w+3fgptSbV4qN1H5FiUjxWRtGidjzilSvhttugSROoUwcWLYKjRz1WjPIj\nvQpIqRCmwQUsAAAS+ElEQVSw+PfFDF48mKSUJMa0HkO7a9p59GYyY+D4cfjXv+Dbb6FMGdu1xMMP\ne6wI5Ud6CkipIGeM4fNNnzN0yVBKFyrNmNZjaFK+icfL2bMHPvoIRoyAmjVtl9OVKkGkd3qxUD6g\nCUCpEHEu5RzT10wnNiGW68tcz6hWo6h9RW2Pl7Njh20gnjfPniJ67jm44QaPF6N8IGTaAJQKd1ER\nUfS5rg9bBmyheYXmtJreil5zerHzyE6PllOpkq0J/PqrbTC++Wbbz1Dq8JQqfGgNQKkAdezMMcZ9\nP46JKyfSo14Phtw8hCsKXeHxcl57zY45cOAANGsG8fFQrJjHi1FeoKeAlApxf5/8m1HLRzHjtxk8\n0fAJnmn6DEXzFfVsGX/Dp5/CpEm2c7l69WDiRI8WobxAE4BSYWLnkZ3EJMTw1bavGNRsEP0b9id/\nVH6PlrFlC6xfDw88AI0b29NFU6Z4tAjlQSGTAGJiYoiOjiY6Otrf4SgV0Nb9vY6hS4ay+q/VxLaI\npce1PYiKiPJoGWvW2FNCHTvaRBARAW+/bcctVv6XkJBAQkICcXFxoZEAAiEOpYLJij0rGLR4EP+c\n/IfnWz3PHTXu8PiANGvWwOHD8Pzz9iayxo1hwgSPFqFyIWRqAIEQh1LBxhjD19u/ZvDiweSNzMuY\n1mNoVamVx8vZuRPWroV77oEHH7R9DLVv7/FiVDZpAlBKkWJS+Hj9xwxbMozKxSszpvUYGpRt4PFy\n5syx/QytXAm9e9s2gg4dPF6McpMmAKXUeUnJSUxZNYX45fE0K9+M+JbxVC9V3aNl7N8Po0ZBYqJN\nCMuXQ4kScOWVHi1GuUETgFLqEolJibz242uMWzGOLtW7EBMdQ7mi5TxaRkoKNG9uLyE1BrZu9ejm\nlRv0TmCl1CUK5inIoJsGseWJLZQqWIpr37yW5xY+x8HEgx4rIyLCdjC3di3s3m17HK1Tx9YKVHDQ\nGoBSYWDv8b3EL4tn9obZPNX4KZ5s/CSF8xb22PZ37oQTJ+y4xNu2wZ13wl132a6plfcEfA1ARCqJ\nyGQR+djbZSml0le2SFneuO0NVjy0gnX/rKPqhKq8/tPrnE0+65HtV6xof/336mXbAp5/3g5EowKb\nz2oAIvKxMeaeDJZpDUApH1q1bxVDlgxh84HNxLeMp1vdbkSI534PPvGE7VoitU+hceP0aiFv8Hkj\nsIhMAW4D9htj6rnMvxV4BVurmGKMeSHN6zQBKBVglu1cxqDFg0hMSmRM6zG0r9LeIzeTnThhxx8A\neOUVKFQI+vWz/Qxd4fn+7MKWPxLATcAJYHpqAhCRCGAL0BrYC6wE7jPGbHJ53WxjzN0ZbFMTgFJ+\nYoxh7ua5DFk8hFIFSzG2zVialm/qse3Png3DhsG5c3Z6+3aPbTrs+eUyUBGpAMxzSQCNgRhjTHtn\nehBgjDEviEgJYBTQBpictmbgrK8JQCk/S05J5v217xOTEEP9K+szqtUo6lxRx2PbP3ECLr8cZs26\nMK9pUyhVymNFhJ3cJgBP9SB1FbDHZfoPoBGAMeYQ0D+rDbgOcKydwinle5ERkfSq34v76tzHGyvf\noPX01rSv0p646DgqFKuQ6+0XKgT33WevFAI7AE2fPjBoUK43HTZSO4HzFE/VALoC7YwxfZ3p7kAj\nY8xAN7enNQClAszR00cZt2Icr698nZ71ejLk5iFcXuhyj23/hRdg9eoLA9SLQJMmUKCAx4oIeYFy\nGeifwNUu0+WceW6LjY31aGZTSuXOZfkvY2TLkWx4bAPnUs5R4/UaxCXEcfzMcY9sv0kTexfx6NH2\ncf/9MHeuRzYd8hISEi46a5JTOa0BVMTWAOo605HAZmwj8D7gJ6CbMWajm9vTGoBSAe73w78zYukI\nFv2+iCE3D6Ffg37ki8rnse0/8gg0bAh9+3pskyHP5zUAEZkJfA9UE5HdItLbGJMMDAAWAuuBj9z9\n8ldKBYfKxSsz484ZfN39a77e/jU1Xq/B+2veJzkl2SPbL1cOHn0U8uSxj88+88hmVSYCpisIHRFM\nqeCyfNdyBi0axImzJxjdejQdq3bM1T0Exly4VPTpp+Gaa+Bf//JQsCFGRwRTSvmdMYZ5W+YxZPEQ\nihcoztjWY2l2dbNcb3f4cFi3zg5JmSoyEu69FwoWzPXmQ0agXAaaa7GxsVoDUCrIiAidqneiY9WO\nzFg7g/s/u59rS1/LqFajqFu6bo6326ED7NsHP/xwYd6CBXYAGv2K8NzloFoDUEp5zOlzp3nz5zcZ\n8+0Y2l3TjpEtR1KxWEWPbLt9exgwQPsUcqUDwiilAs6xM8cY9/04Jq6cSPe63RnafChXFMpdJ0Bd\nu9q7hq+//sK8fPmgRw97eigcBcp9ALmm9wEoFTqK5itKXMs4Nj6+EYOh5us1iU2I5diZYzneZvfu\n9u+vv154/Otf4dm3kF/vA/A0rQEoFdp2HN5BTEIMC7cvZPBNg3n0hkc9cg9BvXowY4b9G45Cpgag\nlApdlYpXYvod01nYYyH/+/1/VJ9Ynelrpuf6HoICBeDUKQ8FGYa0BqCU8rlvdn3DoMWDOHbmGKNb\njea2arfl6B6Cdu3gt98gf/6L5z/+ODzzjIeCDWAh0wisN4IpFV6MMczfMp8hS4ZwWb7LGNtmLDdd\nfVO2tnHsGBw4cPG8Tz6BjRth2jQPBhtg9EYwpVRISE5J5oPfPmDE0hHUuaIOo1uPpl7pnJ/UnzkT\n5s2DDz/0YJABStsAlFJBLTIikp7X9mTzE5u5pfIttH2/LT0+78GOwztytL18+eDMGQ8HGaK0BqCU\nCijHzxxn/IrxvPbTazxQ9wGG3jyU0oVLu/36BQtg4EB730BaBQrYbiaiAqYPhNwJmRqA3geglAIo\nkq8IMdExbHx8IxESQa1JtYhZGuP2PQRNm0L//vamsbSPl1+2YxAEO70PQCkVFnYe2UlMQgxfbfvq\n/D0E+aPyZ/3CdFSoAMuX27+hIGRqAEoplZ6KxSryXpf3WNRjEUt2LKH6xOq8u/rdHN1DkCcPnD3r\nhSCDlCYApVRQqFu6Lv/t9l9m3jmTKaumUO/NeszdNJfsnD3IkweSkrwYZJDRBKCUCirNrm7G8l7L\neaHNCwxbOoxmU5uxfNdyt16bN6/WAFxpG4BSKmglpyTz4boPGb50ODVL1WRM6zFce+W1Ga7fqhV8\n9x1EZPLTt0IF2LTJC8F6gd4JrJQKe2fOneHtX95m1DejaFO5DSNbjqRy8cqXrHfuXOangE6etAng\n5EkvBusBeiewUkqlcfzMcV7+4WVe/fFVutXpxvDmw7N1D8GZM1C0aPDcSKZXASmllKNIviKMaDGC\nTY9vIk9EHmpNqsXwJcM5evqoW6+PirowMH040ASglAo5lxe6nJdvfZlf+/7KnmN7qDaxGuNXjOf0\nudOZvi4iAlJSIFxOSOgpIKVUyFv39zqGLhnK6r9WE9silh7X9iAqIv3+IKKi4PTp4OguImQagQMh\nDqVUaPt+z/cMWjSIA4kHGN16NJ2rd75kHIL8+eHIkUvHGAhEmgCUUiobjDF8ue1LBi8eTME8BRnb\neiwtKrY4v7xwYVi1CgoVcm97UVFwRe7Gu8+x3CaAgKnkxMbG6mWgSimvExE6VO3ArVVu5cPfPqT3\n3N5UL1WdMa3HUP/K+tSrBy1aZL2dVP/8Y0clq1HDezGnlXoZaG5pDUApFdbOJp89fw9By4otiW8Z\nzzUlrnH79Q0awDvvwPXXezHIDOhloEoplQt5I/PyRKMn2DpgKzVL1eTGyTfy+ILH+evEX269PvXK\noWCkCUAppYDCeQszvMVwNj2xiXxR+ag9qTbDlgzL8h6CiAhIzn7HpAFBE4BSSrkoVbAU49uNZ1W/\nVew9vpeqE6oy7vtxGd5DoDUApZQKMVdfdjVTO09l6YNL+XbPt1SbUI2pq6ZyLuXiW4U1ASilVIiq\nfUVtPr/3c2bdNYv31rxH3Tfq8tnGz86PQxAZqQlAKaVCWpPyTUh4MIHxbcczctlIGk9pzNIdS4O6\nDUAvA1VKqWxKMSnMWjeLYUuHcWR7VUa3HkO/Ttf5PI6AvxNYRAoCk4AzwDJjzMx01tEEoJQKOmeT\nz1Kv12T+qRlP2+rRxLeMp0qJKj4rPxjuA7gTmG2M6Qd08kF5SinlE3kj83L1/seYet1W6lxeh8aT\nG9N/fn/2Hd/n79Dcku0EICJTRGS/iKxNM/9WEdkkIltE5P9cFpUD9jjPg/RMmVJKpS8iAvJSmKHN\nh7L5ic0UyluIOm/UYcjiIRw5fcTf4WUqJzWAaUA71xkiEgFMdObXBrqJSGrPGHuwSQAgx1UVpZQK\nRK5XAZUsWJL/tP0Pq/qtYv+J/VSbUI2XvnuJU0mn/BtkBnLUBiAiFYB5xph6znRjIMYY096ZHgQY\nY8wLThvAROAU8K0x5sN0tqdtAEqpoNSpExw6BGXLXrrsWN6NrCs9lEMFVlLr7xgqHulFhJt9cPbr\nB61bZ75OoPQGehUXTvMA/AE0AjDGJAJ9stpAbGzs+efaK6hSKliMGQPr12e0tCbwGVtP/cjMywax\n4tx/uO/yUTQqcucl4xCkVanSpfM81QtoKk/VALoC7YwxfZ3p7kAjY8xAN7enNQClVEgzxrBw+0IG\nLR5Enog8jG0zllaVWuVqm4FyFdCfwNUu0+WceW6LjY31aGZTSqlAIiK0q9KOX/r+wtNNnqbvvL60\nfb8tv+z9JdvbSkhIuOisSY5jymENoCK2BlDXmY4ENgOtgX3AT0A3Y8xGN7enNQClVFhJSk5i8q+T\niV8ez80Vbub5ls9TtWTVbG3D5zUAEZkJfA9UE5HdItLbGJMMDAAWAuuBj9z98k+lNQClVDjJE5mH\n/g37s3XAVq4tfS1NpjTh0fmPsvf43ixf69cagKdpDUApFe4OJh5k7Ldjmbp6Kn2v78u/m/2b4gWK\nZ/qaQGkDUEoplQslC5bkpbYvsebRNRxIPEDCzgSvl6mDwiulVAApV7Qc73R6J9N1dFB4pZQKc3oK\nSCmlVI4ETALQq4CUUso9ehWQUkqFOT0FpJRSKkcCJgHoKSCllHKPngJSSqkwp6eAlFJK5YgmAKWU\nClOaAJRSKkwFTALQRmCllHKPNgIrpVSY00ZgpZRSOaIJQCmlwpQmAKWUClMBkwC0EVgppdyjjcBK\nKRXmtBFYKaVUjmgCUEqpMKUJQCmlwpQmAKWUClOaAJRSKkxpAlBKqTAVMAlA7wNQSin36H0ASikV\n5vQ+AKWUUjmiCUAppcKUJgCllApTmgCUUipMaQJQSqkwpQlAKaXClCYApZQKU15NACJSSUQmi8jH\n3ixHXaA303mW7k/P0v0ZWLyaAIwxO4wxD3uzDHUx/YB5lu5Pz9L9GVjcSgAiMkVE9ovI2jTzbxWR\nTSKyRUT+zzshekZuDjx3X5vVepktz2hZ2vnpreePD1VOy8zO6zy9P92ZF0z7Mruvzen+zM78cNmf\nofJZd7cGMA1o5zpDRCKAic782kA3EanhLOshIuNFpEzq6h6KN8f0oPAsTQCeownAs/Sz7j63+wIS\nkQrAPGNMPWe6MRBjjGnvTA8CjDHmBZfXlABGAW2Aya7L0mxbOwJSSqkcyE1fQFG5KPcqYI/L9B9A\nI9cVjDGHgP5ZbSg3b0AppVTO6GWgSikVpnKTAP4ErnaZLufMU0opFQSykwCEixtzVwJVRKSCiOQF\n7gP+68nglFJKeY+7l4HOBL4HqonIbhHpbYxJBgYAC4H1wEfGmI3eC1UppZQnBcSIYEoppXwvIBuB\nRaSgiLwrIm+JyP3+jifYaZccniUinUXkbRH5UERu8Xc8wUxEaojIGyLysYg86u94QoHz/blSRDpk\nuW4g1gBEpDtw2BizQEQ+Msbc5++YQoGIfGyMucffcYQKESkGvGSMecTfsQQ7ERHgPWNMT3/HEuxE\nJA44DmwwxnyR2bo+qQHkoCuJcly4xyDZFzEGk1DomiOQ5GJ/DgNe902UwSEn+1JEbgfmA5l+WYWj\n7O5PEWkDbAD+wY0eGHx1CihbXUlgv/zLpa7qoxiDSXb35/nVfBNe0Mn2/hSRscAXxpjVvgw0CGR7\nXxpj5hljOgLdfRlokMju/owGbgTuB7LsiNMnCcAY8y1wOM3sRsBWY8wuY0wS8BHQ2Vn2OXCXiLwO\nzPNFjMEku/tTREqIyBtAfa0ZXCoH+3MA0Bp7jPb1abABLgf7soWIvCoibwILfBtt4Mvu/jTGDDPG\nPA18ALyT1fZz0xVEbmXYlYQxJhHo44+gglhm+9OtLjnURTLbnxOACf4IKkhlti+XAcv8EVQQc6cb\nnunubCggrwJSSinlff5MANqVhGfp/vQs3Z+eo/vSszy2P32ZALQrCc/S/elZuj89R/elZ3ltf/rq\nMlDtSsKDdH96lu5Pz9F96Vne3p8BeSOYUkop79NGYKWUClOaAJRSKkxpAlBKqTClCUAppcKUJgCl\nlApTmgCUUipMaQJQSqkwpQlAKaXC1P8DYsy4VJoK9DEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a41a940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "M = COUNTS['the']\n",
    "yscale('log'); xscale('log'); title('Frequency of n-th most frequent word and 1/n line.')\n",
    "plot([c for (w, c) in COUNTS.most_common()])\n",
    "plot([M/i for i in range(1, len(COUNTS)+1)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Task: Spelling Correction\n",
    "\n",
    "<br>\n",
    "Given a word w, find the most likely correction c = correct(w).\n",
    "\n",
    "Approach: Try all candidate words c that are known words that are near w. Choose the most likely one.\n",
    "\n",
    "How to balance near and likely?\n",
    "\n",
    "For now, in a trivial way: always prefer nearer, but when there is a tie on nearness, use the word with the highest WORDS count. Measure nearness by edit distance: the minimum number of deletions, transpositions, insertions, or replacements of characters. By trial and error, we determine that going out to edit distance 2 will give us reasonable results. Then we can define correct(w):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    \"Find the best spelling correction for this word.\"\n",
    "    # Prefer edit distance 0, then 1, then 2; otherwise default to word itself.\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return max(candidates, key=COUNTS.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \"Return the subset of words that are actually in the dictionary.\"\n",
    "    return {w for w in words if w in COUNTS}\n",
    "\n",
    "def edits0(word): \n",
    "    \"Return all strings that are zero edits away from word (i.e., just word itself).\"\n",
    "    return {word}\n",
    "\n",
    "def edits2(word):\n",
    "    \"Return all strings that are two edits away from this word.\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for edits1(word): the set of candidate words that are one edit away. For example, given \"wird\", this would include \"weird\" (inserting an e) and \"word\" (replacing a i with a o), and also \"iwrd\" (transposing w and i; then known can be used to filter this out of the set of final candidates). How could we get them? One way is to split the original word in all possible places, each split forming a pair of words, (a, b), before and after the place, and at each place, either delete, transpose, replace, or insert a letter:\n",
    "\n",
    "|  | | |  |  |  | |\n",
    "| - | - | - | - | - | - | - |\n",
    "| pairs: | Ø+wird | w+ird | wi+rd | wir+d | wird+Ø | Notes: (a, b) pair |\n",
    "| deletions: | Ø+ird | w+rd | wi+d | wir+Ø | | Delete first char of b |\n",
    "| transpositions: | Ø+iwrd | w+rid | wi+dr |  | | Swap first two chars of b |\n",
    "| replacements: | Ø+?ird |w+?rd | wi+?d | wir+? | | Replace char at start of b |\n",
    "| insertions: | Ø+?+wird | w+?+ird | wi+?+rd | wir+?+d | wird+?+Ø | Insert char between a and b |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    \"Return all string that one edit away from this words\"\n",
    "    pairs = splits(word)\n",
    "    deletions = [a + b[1:]                     for a,b in pairs if b]\n",
    "    transposition = [a + b[1] + b[0] + b[2:]   for a,b in pairs if len(b)>1]\n",
    "    replacements = [a + c + b[1:]              for a,b in pairs for c in alphabet]\n",
    "    insertions = [a + c + b                    for a,b in pairs for c in alphabet]\n",
    "    return set(deletions + transposition + replacements + insertions)\n",
    "\n",
    "\n",
    "def splits(word):\n",
    "    \"Return a list of all possible (first, rest) pairs that comprise word.\"\n",
    "    return [(word[:i], word[i:]) for i in range(len(word)+1)]\n",
    "   \n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 'wird'), ('w', 'ird'), ('wi', 'rd'), ('wir', 'd'), ('wird', '')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits('wird')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wird'}\n"
     ]
    }
   ],
   "source": [
    "print(edits0('wird'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rird', 'wirdp', 'wirdo', 'pwird', 'lird', 'wirdb', 'wiru', 'wirdh', 'awird', 'wirdy', 'pird', 'wxrd', 'wdird', 'wmird', 'wtrd', 'wirid', 'gwird', 'mird', 'wirqd', 'wirdk', 'wicd', 'qwird', 'wlrd', 'wirnd', 'qird', 'zird', 'kird', 'wimd', 'wisd', 'wirld', 'werd', 'wirp', 'wirdr', 'wcird', 'wyrd', 'nwird', 'wjird', 'wirod', 'uird', 'gird', 'wicrd', 'wirdj', 'wvird', 'wild', 'aird', 'wirds', 'wrrd', 'wfird', 'wfrd', 'wiqrd', 'wikd', 'wsird', 'wirvd', 'wigrd', 'wizrd', 'wirdn', 'wirbd', 'witrd', 'wirdc', 'wid', 'dird', 'winrd', 'zwird', 'wirrd', 'wixrd', 'wirf', 'wirtd', 'wgrd', 'wxird', 'wihrd', 'word', 'wnrd', 'wikrd', 'wzird', 'tird', 'wirdi', 'wilrd', 'vwird', 'wiord', 'wied', 'bwird', 'whird', 'wigd', 'wirdv', 'wyird', 'wirxd', 'xwird', 'wrird', 'wirm', 'wirh', 'whrd', 'wmrd', 'wirgd', 'wwrd', 'wifd', 'wirdu', 'wqird', 'wire', 'wirn', 'wirq', 'dwird', 'wirpd', 'woird', 'wibd', 'wirhd', 'wirs', 'wdrd', 'wircd', 'cwird', 'wiwrd', 'waird', 'fird', 'jwird', 'wjrd', 'wirdx', 'rwird', 'wind', 'wirjd', 'wirg', 'wbird', 'wirv', 'bird', 'wnird', 'wrid', 'wirmd', 'wijd', 'fwird', 'wiyd', 'wiqd', 'wirj', 'widd', 'wirdq', 'wirdz', 'wihd', 'widrd', 'wirwd', 'wiro', 'iwrd', 'wiad', 'witd', 'wibrd', 'wiwd', 'wkrd', 'wwird', 'ird', 'iwird', 'cird', 'wirzd', 'nird', 'wirb', 'wuird', 'wirad', 'widr', 'iird', 'wirw', 'wiprd', 'wiud', 'wirud', 'wprd', 'wisrd', 'oird', 'wsrd', 'wiid', 'wirx', 'wirz', 'wzrd', 'wcrd', 'ywird', 'wiry', 'wtird', 'jird', 'wiyrd', 'lwird', 'eird', 'wirdf', 'wirk', 'wirdt', 'wiod', 'wir', 'wbrd', 'kwird', 'weird', 'twird', 'wiurd', 'wijrd', 'wizd', 'wierd', 'wirc', 'hird', 'wirl', 'wirdm', 'mwird', 'wirkd', 'ward', 'wirt', 'wirdd', 'wifrd', 'wirsd', 'wirr', 'uwird', 'owird', 'wira', 'wimrd', 'wqrd', 'wirda', 'wrd', 'wivd', 'wvrd', 'wiri', 'wlird', 'wpird', 'wgird', 'wiard', 'swird', 'wired', 'wiryd', 'wixd', 'wirdl', 'wkird', 'wirde', 'wivrd', 'hwird', 'vird', 'wirdw', 'wipd', 'xird', 'yird', 'wird', 'wiird', 'ewird', 'wirdg', 'wurd', 'sird', 'wirfd'}\n"
     ]
    }
   ],
   "source": [
    "print(edits1('wird'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24254\n"
     ]
    }
   ],
   "source": [
    "print(len(edits2('wird')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seeing',\n",
       " 'error',\n",
       " 'in',\n",
       " 'something',\n",
       " 'whatever',\n",
       " 'unusual',\n",
       " 'mistaken',\n",
       " 'everywhere']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(correct, tokens('Speling errurs in somethink. Whutever; unusuel misteakes everyware?')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we make the output prettier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'Speling errurs in somethink. Whutever; unusuel misteakes everyware?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_text(text):\n",
    "    \"Correct all the words within a text, returning the corrected text.\"\n",
    "    return re.sub('[a-zA-Z]+', correct_match, text)\n",
    "\n",
    "def correct_match(match):\n",
    "    \"Spell-correct word in match, and preserve proper upper/lower/title case.\"\n",
    "    word = match.group()\n",
    "    return case_of(word)(correct(word.lower()))\n",
    "\n",
    "def case_of(text):\n",
    "    \"Return the case-function appropriate for text: upper, lower, title, or just str.\"\n",
    "    return (str.upper if text.isupper() else\n",
    "            str.lower if text.islower() else\n",
    "            str.title if text.istitle() else\n",
    "            str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<method 'upper' of 'str' objects>,\n",
       " <method 'lower' of 'str' objects>,\n",
       " <method 'title' of 'str' objects>,\n",
       " str]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(case_of, ['UPPER', 'lower', 'Title', 'CamelCase']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seeing Error IN something. Whatever; unusual mistaken?'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_text('Speling Errurs IN somethink. Whutever; unusuel misteakes?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Advance says: tumbler ...'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_text('Audiance sayzs: tumblr ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good. You can probably think of a dozen ways to make this better. Here's one: in the text \"three, too, one, blastoff!\" we might want to correct \"too\" with \"two\", even though \"too\" is in the dictionary. We can do better if we look at a sequence of words, not just an individual word one at a time. But how can we choose the best corrections of a sequence? The ad-hoc approach worked pretty well for single words, but now we could use some real theory ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Theory: From Counts to Probabilities of Word Sequences\n",
    "<br>\n",
    "We should be able to compute the probability of a word, P(w)P(w). We do that with the function pdist, which takes as input a Counter (hat is, a bag of words) and returns a function that acts as a probability distribution over all possible words. In a probability distribution the probability of each word is between 0 and 1, and the sum of the probabilities is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106274\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "def pdist(counter):\n",
    "    \n",
    "    \"Make a probability distribution, given evidence from a Counter.\"\n",
    "    # N = sum(counter.values())\n",
    "    N = sum([i for i in counter.values()])\n",
    "    print(N)\n",
    "    print(counter['word'])\n",
    "    return lambda x: counter[x]/N\n",
    "\n",
    "P = pdist(COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0532021002315 the\n",
      "0.0106046634172 is\n",
      "0.000856277170333 most\n",
      "0.000131734949282 common\n",
      "0.000376385569377 word\n",
      "0.0166644710842 in\n",
      "9.40963923443e-05 english\n"
     ]
    }
   ],
   "source": [
    "for w in tokens('\"The\" is most common word in English'):\n",
    "    print(P(w), w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what is the probability of a sequence of words? Use the definition of a joint probability:\n",
    "\n",
    "$$ P(w_1 \\ldots w_n) = P(w_1) \\times P(w_2 \\mid w_1) \\times P(w_3 \\mid w_1 w_2) \\times \\ldots \\times P(w_n \\mid w_1 \\ldots w_{n-1} ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag of words model assumes that each word is drawn from the bag independently of the others. This gives us the wrong approximation:\n",
    "\n",
    "$$ P(w_1 \\ldots w_n) = P(w_1) \\times P(w_2) \\times P(w_3) \\times \\ldots \\times P(w_n ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistician George Box said that All models are wrong, but some are useful.\n",
    "\n",
    "How can we compute $P(w_1 \\ldots w_n)$? We'll use a different function name, $\\text{Pwords}$, rather than $\\text{P}$, and we compute the product of the individual probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Pwords(words):\n",
    "    \"Probability of words, assuming each word is independent of others.\"\n",
    "    return product(P(w) for w in words)\n",
    "\n",
    "def product(nums):\n",
    "    \"Multiply the numbers together.  (Like `sum`, but with multiplication.)\"\n",
    "    result = 1\n",
    "    for x in nums:\n",
    "        result *= x\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.93183071733e-11 this is a test\n",
      "5.21808210274e-15 this is a unusual test\n",
      "0.0 this is a neverbeforeseen test\n"
     ]
    }
   ],
   "source": [
    "tests = ['this is a test', \n",
    "         'this is a unusual test',\n",
    "         'this is a neverbeforeseen test']\n",
    "\n",
    "for test in tests:\n",
    "    print(Pwords(tokens(test)), test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes—it seems wrong to give a probability of 0 to the last one; it should just be very small. We'll come back to that later. The other probabilities seem reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Task: Word Segmentation\n",
    "\n",
    "**Task:** <br>\n",
    "Given a sequence of characters with no spaces separating words, recover the sequence of words.\n",
    "\n",
    "Why? Languages with no word delimiters: 不带空格的词. <br>\n",
    "In English, sub-genres with no word delimiters (spelling errors, URLs).\n",
    "\n",
    "**Approach 1:** <br>\n",
    "Enumerate all candidate segmentations and choose the one with highest Pwords\n",
    "\n",
    "Problem: how many segmentations are there for an n-character text?\n",
    "\n",
    "**Approach 2: ** <br>\n",
    "Make one segmentation, into a first word and remaining characters. If we assume words are independent then we can maximize the probability of the first word adjoined to the best segmentation of the remaining characters.\n",
    "\n",
    "    assert segment('choosespain') == ['choose', 'spain']\n",
    "\n",
    "    segment('choosespain') ==\n",
    "       max(Pwords(['c'] + segment('hoosespain')),\n",
    "           Pwords(['ch'] + segment('oosespain')),\n",
    "           Pwords(['cho'] + segment('osespain')),\n",
    "           Pwords(['choo'] + segment('sespain')),\n",
    "           ...\n",
    "           Pwords(['choosespain'] + segment('')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this somewhat efficient, we need to avoid re-computing the segmentations of the remaining characters. This can be done explicitly by dynamic programming or implicitly with memoization. Also, we shouldn't consider all possible lengths for the first word; we can impose a maximum length. What should it be? A little more than the longest word seen so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def memo(f):\n",
    "    \"Memoize function f, whose args must all be hashable.\"\n",
    "    cache = {}\n",
    "    def fmemo(*args):\n",
    "        if args not in cache:\n",
    "            cache[args] = f(*args)\n",
    "        return cache[args]\n",
    "    fmemo.cache = cache\n",
    "    return fmemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(w) for w in COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[66:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def splits(text, start = 0, L = 20):\n",
    "    \"Return a list of all (first, rest) pairs; start <= len(first) <= L.\"\n",
    "    return [(text[:i], text[i:]) for i in range(start, min(len(text), L)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 'word'), ('w', 'ord'), ('wo', 'rd'), ('wor', 'd'), ('word', '')]\n",
      "[('r', 'eallylongtext'), ('re', 'allylongtext'), ('rea', 'llylongtext'), ('real', 'lylongtext')]\n"
     ]
    }
   ],
   "source": [
    "print(splits('word'))\n",
    "print(splits('reallylongtext', 1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@memo\n",
    "def segment(text):\n",
    "    \"Return a list of words that is the most probable segmentation of text.\"\n",
    "    if not text:\n",
    "        return []\n",
    "    else:\n",
    "        candidates = ([first] + segment(rest) \n",
    "                      for (first, rest) in splits(text,1))\n",
    "        return max(candidates, key = Pwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['choose', 's', 'pain']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment('choosespain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['speed', 'of', 'art']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment('speedofart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decl = ('wheninthecourseofhumaneventsitbecomesnecessaryforonepeople' +\n",
    "        'todissolvethepoliticalbandswhichhaveconnectedthemwithanother' +\n",
    "        'andtoassumeamongthepowersoftheearththeseparateandequalstation' +\n",
    "        'towhichthelawsofnatureandofnaturesgodentitlethem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'in', 'the', 'course', 'of', 'human', 'events', 'it', 'becomes', 'necessary', 'for', 'one', 'people', 'to', 'd', 'is', 'solve', 'the', 'political', 'bands', 'which', 'have', 'connected', 'them', 'with', 'another', 'and', 'to', 'assume', 'among', 'the', 'powers', 'of', 'the', 'earth', 'the', 'separate', 'and', 'equal', 'station', 'to', 'which', 'the', 'laws', 'of', 'nature', 'and', 'of', 'nature', 's', 'go', 'den', 'title', 'them']\n"
     ]
    }
   ],
   "source": [
    "print(segment(decl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.6691021676551711e-154"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pwords(segment(decl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.5153332393243606e-307"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pwords(segment(decl * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pwords(segment(decl * 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a problem. We'll come back to it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['small', 'and', 'in', 'significant']"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment('smallandinsignificant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['large', 'and', 'in', 'significant']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment('largeandinsignificant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.72137739809e-12\n"
     ]
    }
   ],
   "source": [
    "print(Pwords(['large', 'and', 'insignificant']))\n",
    "print(Pwords(['large', 'and', 'in', 'significant']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "* Looks pretty good!\n",
    "* The bag-of-words assumption is a limitation.\n",
    "* Recomputing Pwords on each recursive call is somewhat inefficient.\n",
    "* Numeric underflow for texts longer than 100 or so words; we'll need to use logarithms, or other tricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Data: Mo' Data, Mo' Better\n",
    "\n",
    "Let's move up from millions to billions and billions of words. Once we have that amount of data, we can start to look at two word sequences, without them being too sparse. \n",
    "<br>I happen to have data files available in the format of \"`word \\t count`\", and bigram data in the form of \"`word1 word2 \\t count`\". Let's arrange to read them in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_counts(filename, sep='\\t'):\n",
    "    \"\"\"Return a Counter initialized from key-value pairs, \n",
    "    one on each line of filename.\"\"\"\n",
    "    C = Counter()\n",
    "    for line in open(filename):\n",
    "        key, count = line.split(sep)\n",
    "        C[key] = int(count)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588124220187\n",
      "98671341\n",
      "225955251755\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "COUNTS1 = load_counts('nlp_data/count_1w.txt')\n",
    "COUNTS2 = load_counts('nlp_data/count_2w.txt')\n",
    "\n",
    "P1w = pdist(COUNTS1)\n",
    "P2w = pdist(COUNTS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333333 588.124220187\n",
      "286358 225.955251755\n"
     ]
    }
   ],
   "source": [
    "print(len(COUNTS1), sum([i for i in COUNTS1.values()])/1e9)\n",
    "print(len(COUNTS2), sum([i for i in COUNTS2.values()])/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of the', 2766332391),\n",
       " ('in the', 1628795324),\n",
       " ('to the', 1139248999),\n",
       " ('on the', 800328815),\n",
       " ('for the', 692874802),\n",
       " ('and the', 629726893),\n",
       " ('to be', 505148997),\n",
       " ('is a', 476718990),\n",
       " ('with the', 461331348),\n",
       " ('from the', 428303219),\n",
       " ('by the', 417106045),\n",
       " ('at the', 416201497),\n",
       " ('of a', 387060526),\n",
       " ('in a', 364730082),\n",
       " ('will be', 356175009),\n",
       " ('that the', 333393891),\n",
       " ('do not', 326267941),\n",
       " ('is the', 306482559),\n",
       " ('to a', 279146624),\n",
       " ('is not', 276753375),\n",
       " ('for a', 274112498),\n",
       " ('with a', 271525283),\n",
       " ('as a', 270401798),\n",
       " ('<S> and', 261891475),\n",
       " ('of this', 258707741),\n",
       " ('<S> the', 258483382),\n",
       " ('it is', 245002494),\n",
       " ('can be', 230215143),\n",
       " ('If you', 210252670),\n",
       " ('has been', 196769958)]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COUNTS2.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Theory and Practice: Segmentation With Bigram Data\n",
    "\n",
    "A less-wrong approximation:\n",
    "\n",
    "$$ P(w_1 \\ldots w_n) = P(w_1) \\times P(w_2 \\mid w_1) \\times P(w_3 \\mid w_2) \\times \\ldots \\times P(w_n \\mid w_{n-1} ) $$\n",
    "\n",
    "This is called the **bigram model**, and is equivalent to taking a text, cutting it up into slips of paper with two words on them, and having multiple bags, and putting each slip into a bag labelled with the first word on the slip. Then, to generate language, we choose the first word from the original single bag of words, and chose all subsequent words from the bag with the label of the previously-chosen word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining the probability of a single discrete event, given evidence stored in a Counter:\n",
    "\n",
    "Recall that the less-wrong bigram model approximation to English is:\n",
    "\n",
    "$ P(w_1 \\ldots w_n) = P(w_1) \\times P(w_2 \\mid w_1) \\times P(w_3 \\mid w_2) \\times \\ldots \\times P(w_n \\mid w_{n-1} ) $\n",
    "\n",
    "where the conditional probability of a word given the previous word is defined as:\n",
    "\n",
    "$ P(w_n \\mid w_{n-1}) = \\frac{P(w_{n-1} w_n)}{P(w_{n-1})} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'test']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for i,w in enumerate(tokens('this is a test'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Pwords2(words, prev= '<S>'):\n",
    "    \"The probability of a sequence of words, using bigram data, given prev word.\"\n",
    "    return product(cPword(w, prev if (i==0) else words[i-1]) for i, w in enumerate(words))\n",
    "\n",
    "# Change Pwords to use P1w (the bigger dictionary) instead of Pword\n",
    "def Pwords(words):\n",
    "    \"Probability of words, assuming each word is independent of others.\"\n",
    "    return product(P1w(w) for w in words)\n",
    "\n",
    "def cPword(word, prev):\n",
    "    \"Conditional probability of word, given previous word.\"\n",
    "    bigram = prev + ' ' + word\n",
    "    if P2w(bigram) > 0 and P1w(prev) > 0:\n",
    "        return P2w(bigram)/P1w(prev)\n",
    "    else: # Average the back-off value and zero.\n",
    "        return P1w(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.78739820006e-10\n",
      "1.28273525888e-07\n",
      "9.44228802937e-11\n"
     ]
    }
   ],
   "source": [
    "print(Pwords(tokens('this is a test')))\n",
    "print(Pwords2(tokens('this is a test')))\n",
    "print(Pwords2(tokens('is test a this')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make `segment2`, we copy `segment`, and make sure to pass around the previous token, and to evaluate probabilities with `Pwords2` instead of `Pwords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@memo\n",
    "def segment2(text, prev='<S>'):\n",
    "    \"Return best segmentation of text; use bigram data.\" \n",
    "    if not text:\n",
    "        return []\n",
    "    else:\n",
    "        candidates = ([first] + segment2(rest) \n",
    "                      for (first, rest) in splits(text,1))\n",
    "        return max(candidates, key = lambda words: Pwords2(words, prev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['choose', 'spain']\n",
      "['speed', 'of', 'art']\n",
      "['small', 'and', 'insignificant']\n",
      "['large', 'and', 'insignificant']\n"
     ]
    }
   ],
   "source": [
    "print(segment2('choosespain'))\n",
    "print(segment2('speedofart'))\n",
    "print(segment2('smallandinsignificant'))\n",
    "print(segment2('largeandinsignificant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['far', 'out', 'in', 'the', 'uncharted', 'backwaters', 'of', 'the', 'unfashionable', 'end', 'of', 'the', 'western', 'spiral', 'arm', 'of', 'the', 'galaxy', 'lies', 'a', 'small', 'un', 'regarded', 'yellow', 'sun']\n",
      "['far', 'out', 'in', 'the', 'uncharted', 'backwaters', 'of', 'the', 'unfashionable', 'end', 'of', 'the', 'western', 'spiral', 'arm', 'of', 'the', 'galaxy', 'lies', 'a', 'small', 'un', 'regarded', 'yellow', 'sun']\n"
     ]
    }
   ],
   "source": [
    "adams = ('faroutintheunchartedbackwatersoftheunfashionableendofthewesternspiral' +\n",
    "         'armofthegalaxyliesasmallunregardedyellowsun')\n",
    "print(segment(adams))\n",
    "print(segment2(adams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P1w('unregarded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'dry', 'bare', 'sandy', 'hole', 'with', 'nothing', 'in', 'it', 'to', 'sitdown', 'on', 'or', 'to', 'eat']\n",
      "['a', 'dry', 'bare', 'sandy', 'hole', 'with', 'nothing', 'in', 'it', 'to', 'sit', 'down', 'on', 'or', 'to', 'eat']\n"
     ]
    }
   ],
   "source": [
    "tolkein = 'adrybaresandyholewithnothinginittositdownonortoeat'\n",
    "print(segment(tolkein))\n",
    "print(segment2(tolkein))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion?\n",
    "Bigram model is a little better, but not much. Hundreds of billions of words still not enough. (Why not trillions?) Could be made more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Theory: Evaluation\n",
    "\n",
    "So far, we've got an intuitive feel for how this all works. But we don't have any solid metrics that quantify the results. Without metrics, we can't say if we are doing well, nor if a change is an improvement. In general, when developing a program that relies on data to help make predictions, it is good practice to divide your data into three sets:\n",
    "\n",
    "1. **Training set**: the data used to create our spelling model; this was the big.txt file.\n",
    "2. **Development set**: a set of input/output pairs that we can use to rank the performance of our program as we are developing it.\n",
    "3. **Test set**: another set of input/output pairs that we use to rank our program after we are done developing it. The development set can't be used for this purpose—once the programmer has looked at the development test it is tainted, because the programmer might modify the program just to pass the development test. That's why we need a separate test set that is only looked at after development is done.\n",
    "\n",
    "For this program, the training data is the word frequency counts, the development set is the examples like \"choosespain\" that we have been playing with, and now we need a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_segmenter(segmenter, tests):\n",
    "    \"Try segmenter on tests; report failures; return fraction correct.\"\n",
    "    return sum([test_one_segment(segmenter, test) \n",
    "        for test in tests]), len(tests)\n",
    "\n",
    "def test_one_segment(segmenter, test):\n",
    "    words = tokens(test)\n",
    "    result = segmenter(''.join(words))\n",
    "    correct = (result == words)\n",
    "    if not correct:\n",
    "        print('expected', words)\n",
    "        print('got     ', result)\n",
    "    return correct\n",
    "\n",
    "proverbs = (\"\"\"A little knowledge is a dangerous thing\n",
    "  A man who is his own lawyer has a fool for his client\n",
    "  All work and no play makes Jack a dull boy\n",
    "  Better to remain silent and be thought a fool that to speak and remove all doubt;\n",
    "  Do unto others as you would have them do to you\n",
    "  Early to bed and early to rise, makes a man healthy, wealthy and wise\n",
    "  Fools rush in where angels fear to tread\n",
    "  Genius is one percent inspiration, ninety-nine percent perspiration\n",
    "  If you lie down with dogs, you will get up with fleas\n",
    "  Lightning never strikes twice in the same place\n",
    "  Power corrupts; absolute power corrupts absolutely\n",
    "  Here today, gone tomorrow\n",
    "  See no evil, hear no evil, speak no evil\n",
    "  Sticks and stones may break my bones, but words will never hurt me\n",
    "  Take care of the pence and the pounds will take care of themselves\n",
    "  Take care of the sense and the sounds will take care of themselves\n",
    "  The bigger they are, the harder they fall\n",
    "  The grass is always greener on the other side of the fence\n",
    "  The more things change, the more they stay the same\n",
    "  Those who do not learn from history are doomed to repeat it\"\"\"\n",
    "  .splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected ['sticks', 'and', 'stones', 'may', 'break', 'my', 'bones', 'but', 'words', 'will', 'never', 'hurt', 'me']\n",
      "got      ['stick', 'sandstones', 'may', 'break', 'my', 'bones', 'but', 'words', 'will', 'never', 'hurt', 'me']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19, 20)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_segmenter(segment, proverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_segmenter(segment2, proverbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that both segmenters are very good, and that segment2 is slightly better. There is much more that can be done in terms of the variety of tests, and in measuring statistical significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - Theory and Practice: Smoothing\n",
    "Let's go back to a test we did before, and add some more test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.78739820006e-10 this is a test\n",
      "3.78675425278e-15 this is a unusual test\n",
      "1.31179474235e-16 this is a nongovernmental test\n",
      "0.0 this is a neverbeforeseen test\n",
      "0.0 this is a zqbhjhsyefvvjqc test\n"
     ]
    }
   ],
   "source": [
    "tests = ['this is a test', \n",
    "         'this is a unusual test',\n",
    "         'this is a nongovernmental test',\n",
    "         'this is a neverbeforeseen test',\n",
    "         'this is a zqbhjhsyefvvjqc test']\n",
    "\n",
    "for test in tests:\n",
    "    print(Pwords(tokens(test)), test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue here is the finality of a probability of zero. Out of the three 15-letter words, it turns out that \"nongovernmental\" is in the dictionary, but if it hadn't been, if somehow our corpus of words had missed it, then the probability of that whole phrase would have been zero. It seems that is too strict; there must be some \"real\" words that are not in our dictionary, so we shouldn't give them probability zero. There is also a question of likelyhood of being a \"real\" word. It does seem that \"neverbeforeseen\" is more English-like than \"zqbhjhsyefvvjqc\", and so perhaps should have a higher probability.\n",
    "\n",
    "We can address this by assigning a non-zero probability to words that are not in the dictionary. This is even more important when it comes to multi-word phrases (such as bigrams), because it is more likely that a legitimate one will appear that has not been observed before.\n",
    "\n",
    "We can think of our model as being overly spiky; it has a spike of probability mass wherever a word or phrase occurs in the corpus. What we would like to do is smooth over those spikes so that we get a model that does not depend on the details of our corpus. The process of \"fixing\" the model is called **smoothing**.\n",
    "\n",
    "For example, Laplace was asked what's the probability of the sun rising tomorrow. From data that it has risen $n/n$ times for the last $n$ days, the maximum liklihood estimator is 1. But Laplace wanted to balance the data with the possibility that tomorrow, either it will rise or it won't, so he came up with $(n+1)/(n+2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588124220187\n",
      "98671341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P1w = pdist(COUNTS1)\n",
    "P1w('neverbeenseenbefore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the previous probability without smoothing, then we get 0 probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pdist_additive_smoothed(counter, c=1):\n",
    "    \"\"\"The probability of word, given evidence from the counter.\n",
    "    Add c to the count for each item, plus the 'unknown' item.\"\"\"\n",
    "    N = sum([i for i in counter.values()])          # Amount of evidence\n",
    "    Nplus = N + c * (len(counter) + 1) # Evidence plus fake observations\n",
    "    return lambda word: (counter[word] + c) / Nplus \n",
    "\n",
    "P1w = pdist_additive_smoothed(COUNTS1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7003201005861308e-12"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P1w('neverbeenseenbefore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, after we added some smoothing process, we get probability 1.7003201005861308e-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now there's a problem ... we now have previously-unseen words with non-zero probabilities. And maybe $10^{-12}$ is about right for words that are observed in text: that is, if I'm reading a new text, the probability that the next word is unknown might be around $10^{-12}$. But if I'm manufacturing 20-letter sequences at random, the probability that one will be a word is much, much lower than $10^{-12}$.\n",
    "\n",
    "Look what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thisisatestofsegment', 'ationofalongsequence', 'of', 'words']"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment('thisisatestofsegmentationofalongsequenceofwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two problems:\n",
    "\n",
    "First, we don't have a clear model of the unknown words. We just say \"unknown\" but we don't distinguish likely unknown from unlikely unknown. For example, is a 8-character unknown more likely than a 20-character unknown?\n",
    "\n",
    "Second, we don't take into account evidence from parts of the unknown. For example, \"unglobulate\" versus \"zxfkogultae\".\n",
    "\n",
    "For our next approach, Good - Turing smoothing re-estimates the probability of zero-count words, based on the probability of one-count words (and can also re-estimate for higher-number counts, but that is less interesting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how many one-count words are there in COUNTS? (There aren't any in COUNTS1.) And what are the word lengths of them? Let's find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 620),\n",
       " (8, 598),\n",
       " (6, 532),\n",
       " (9, 448),\n",
       " (5, 396),\n",
       " (10, 300),\n",
       " (4, 209),\n",
       " (11, 169),\n",
       " (12, 85),\n",
       " (3, 52),\n",
       " (13, 36),\n",
       " (14, 20),\n",
       " (2, 6),\n",
       " (1, 2),\n",
       " (15, 2),\n",
       " (17, 2),\n",
       " (18, 1)]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singletons = (w for w in COUNTS if COUNTS[w] == 1)\n",
    "\n",
    "lengths = map(len, singletons)\n",
    "\n",
    "Counter(lengths).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x115463b00>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012768880441123887"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1357 / sum([i for i in COUNTS.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "matplotlib does not support generators as input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-278-a23d0434660e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/ellen/anaconda/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(x, bins, range, normed, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   2956\u001b[0m                       \u001b[0mhisttype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhisttype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2957\u001b[0m                       \u001b[0mrwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m                       stacked=stacked, data=data, **kwargs)\n\u001b[0m\u001b[1;32m   2959\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwashold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ellen/anaconda/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1810\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1811\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1812\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1813\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ellen/anaconda/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(self, x, bins, range, normed, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)\u001b[0m\n\u001b[1;32m   5958\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5959\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5960\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_normalize_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5961\u001b[0m         \u001b[0mnx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# number of datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ellen/anaconda/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36m_normalize_input\u001b[0;34m(inp, ename)\u001b[0m\n\u001b[1;32m   5889\u001b[0m             \"\"\"\n\u001b[1;32m   5890\u001b[0m             if (isinstance(x, np.ndarray) or\n\u001b[0;32m-> 5891\u001b[0;31m                     not iterable(cbook.safe_first_element(inp))):\n\u001b[0m\u001b[1;32m   5892\u001b[0m                 \u001b[0;31m# TODO: support masked arrays;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5893\u001b[0m                 \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ellen/anaconda/lib/python3.5/site-packages/matplotlib/cbook.py\u001b[0m in \u001b[0;36msafe_first_element\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2540\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msafe_first_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2542\u001b[0;31m         raise RuntimeError(\"matplotlib does not support generators \"\n\u001b[0m\u001b[1;32m   2543\u001b[0m                            \"as input\")\n\u001b[1;32m   2544\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: matplotlib does not support generators as input"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqbH/BTlbs8dE2Xm9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115463080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist(lengths, bins=len(set(lengths)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pdist_good_turing_hack(counter, onecounter, base=1/26., prior=1e-8):\n",
    "    \"\"\"The probability of word, given evidence from the counter.\n",
    "    For unknown words, look at the one-counts from onecounter, based on length.\n",
    "    This gets ideas from Good-Turing, but doesn't implement all of it.\n",
    "    prior is an additional factor to make unknowns less likely.\n",
    "    base is how much we attenuate probability for each letter beyond longest.\"\"\"\n",
    "    N = sum([i for i in counter.values()])\n",
    "    N2 = sum([i for i in onecounter.values()])\n",
    "    lengths = map(len, [w for w in onecounter if onecounter[w] == 1])\n",
    "    ones = Counter(lengths)\n",
    "    longest = max(ones)\n",
    "    return (lambda word: \n",
    "            counter[word] / N if (word in counter) \n",
    "            else prior * (ones[len(word)] / N2 or \n",
    "                          ones[longest] / N2 * base ** (len(word)-longest)))\n",
    "\n",
    "# Redefine P1w\n",
    "P1w = pdist_good_turing_hack(COUNTS1, COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'isatestofsegment', 'ationofaverylong', 'sequence', 'of', 'words']"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment.cache.clear()\n",
    "segment('thisisatestofsegmentationofaverylongsequenceofwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, I set the prior to 1e-8. And the result is not good. So, I changed the prior to 1e-12 and the result is better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " 'of',\n",
       " 'segmentation',\n",
       " 'of',\n",
       " 'a',\n",
       " 'very',\n",
       " 'long',\n",
       " 'sequence',\n",
       " 'of',\n",
       " 'words']"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redefine P1w\n",
    "P1w = pdist_good_turing_hack(COUNTS1, COUNTS, prior = 1e-12)\n",
    "segment.cache.clear()\n",
    "segment('thisisatestofsegmentationofaverylongsequenceofwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was somewhat unsatisfactory. We really had to crank up the prior, specifically because the process of running segment generates so many non-word candidates (and also because there will be fewer unknowns with respect to the billion-word WORDS1 than with respect to the million-word WORDS). It would be better to separate out the prior from the word distribution, so that the same distribution could be used for multiple tasks, not just for this one.\n",
    "\n",
    "Now let's think for a short while about smoothing bigram counts. Specifically, what if we haven't seen a bigram sequence, but we've seen both words individually? For example, to evaluate P(\"Greenland\") in the phrase \"turn left at Greenland\", we might have three pieces of evidence:\n",
    "\n",
    "    P(\"Greenland\")\n",
    "    P(\"Greenland\" | \"at\")\n",
    "    P(\"Greenland\" | \"left\", \"at\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presumably, the first would have a relatively large count, and thus large reliability, while the second and third would have decreasing counts and reliability. With interpolation smoothing we combine all three pieces of evidence, with a linear combination:\n",
    "\n",
    "$P(w_3 \\mid w_1 w_2)=c_1 P(w_3)+c_2 P(w_3 \\mid w_2) + c_3 P(w_3 \\mid w_1w_2)$\n",
    "\n",
    "How do we choose $c_1,c_2,c_3$? By experiment: train on training data, maximize cc values on development data, then evaluate on test data.\n",
    "\n",
    "However, when we do this, we are saying, with probability c1c1, that a word can appear anywhere, regardless of previous words. But some words are more free to do that than other words. Consider two words with similar probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.73314623661e-05\n",
      "7.72494966889e-05\n"
     ]
    }
   ],
   "source": [
    "print(P1w('francisco'))\n",
    "print(P1w('individuals'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They have similar unigram probabilities but differ in their freedom to be the second word of a bigram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['San francisco', 'san francisco']\n"
     ]
    }
   ],
   "source": [
    "print([bigram for bigram in COUNTS2 if bigram.endswith('francisco')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['or individuals', 'infected individuals', 'following individuals', 'the individuals', 'are individuals', 'which individuals', 'as individuals', 'more individuals', 'to individuals', 'for individuals', 'both individuals', 'minded individuals', 'many individuals', 'that individuals', 'on individuals', 'interested individuals', 'such individuals', 'those individuals', 'private individuals', 'qualified individuals', 'these individuals', 'in individuals', 'about individuals', 'two individuals', 'by individuals', 'and individuals', 'where individuals', '<S> individuals', 'certain individuals', 'For individuals', 'other individuals', 'healthy individuals', 'some individuals', 'among individuals', 'all individuals', 'from individuals', 'different individuals', 'between individuals', 'few individuals', 'with individuals', 'of individuals', 'These individuals', 'affected individuals', 'help individuals', 'income individuals']\n"
     ]
    }
   ],
   "source": [
    "print([bigram for bigram in COUNTS2 if bigram.endswith('individuals')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, words that appear in many bigrams before are more likely to appear in a new, previously unseen bigram. In Kneser-Ney smoothing (Reinhard Kneser, Hermann Ney) we multiply the bigram counts by this ratio. But I won't implement that here, because The Count never covered it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - One More Task: Secret Codes\n",
    "\n",
    "Let's tackle one more task: decoding secret codes. We'll start with the simplest of codes, a rotation cipher, sometimes called a shift cipher or a Caesar cipher (because this was state-of-the-art crypotgraphy in 100 BC). First, a method to encode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rot(msg, n=13): \n",
    "    \"Encode a message with a rotation (Caesar) cipher.\" \n",
    "    return encode(msg, alphabet[n:]+alphabet[:n])\n",
    "\n",
    "def encode(msg, key): \n",
    "    \"Encode a message with a substitution cipher.\" \n",
    "    table = str.maketrans(upperlower(alphabet), upperlower(key))\n",
    "    return msg.translate(table) \n",
    "\n",
    "def upperlower(text): return text.upper() + text.lower()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Uijt jt b tfdsfu nfttbhf.'"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rot('This is a secret message.', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Guvf vf n frperg zrffntr.'"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rot('This is a secret message.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a secret message.'"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rot(rot('This is a secret message.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now decoding is easy: try all 26 candidates, and find the one with the maximum Pwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_rot(secret):\n",
    "    \"Decode a secret message that has been encoded with a rotation cipher.\"\n",
    "    candidates = [rot(secret, i) for i in range(len(alphabet))]\n",
    "    return max(candidates, key=lambda msg: Pwords(tokens(msg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nyf befnj kyv rejnvi?\n",
      "Who knows the answer?\n"
     ]
    }
   ],
   "source": [
    "msg = 'Who knows the answer?'\n",
    "secret = rot(msg, 17)\n",
    "\n",
    "print(secret)\n",
    "print(decode_rot(secret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make it a tiny bit harder. When the secret message contains separate words, it is too easy to decode by guessing that the one-letter words are most likely \"I\" or \"a\". So what if the encode routine mushed all the letters together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(msg, key): \n",
    "    \"Encode a message with a substitution cipher; remove non-letters.\" \n",
    "    msg = ''.join(tokens(msg))  ## Change here\n",
    "    table = str.maketrans(upperlower(alphabet), upperlower(key))\n",
    "    return msg.translate(table) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can decode by segmenting. We change candidates to be a list of segmentations, and still choose the candidate with the best Pwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_rot(secret):\n",
    "    \"\"\"Decode a secret message that has been encoded with a rotation cipher,\n",
    "    and which has had all the non-letters squeezed out.\"\"\"\n",
    "    candidates = [segment(rot(secret, i)) for i in range(len(alphabet))]\n",
    "    return max(candidates, key=lambda msg: Pwords(msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pahdghplmaxtglpxkmablmbfxtgrhgxunxeexk\n",
      "['who', 'knows', 'the', 'answer', 'this', 'time', 'anyone', 'bueller']\n"
     ]
    }
   ],
   "source": [
    "msg = 'Who knows the answer this time? Anyone? Bueller?'\n",
    "secret = rot(msg, 19)\n",
    "\n",
    "print(secret)\n",
    "print(decode_rot(secret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pahdghplmaxtglpxkm', 'ablmbfxtgrhgxunxeexk'] 1.30978270003e-37\n",
      "['qbiehiqmnbyuhmqyln', 'bcmncgyuhsihyvoyffyl'] 1.30978270003e-37\n",
      "['rcjfijrnoczvinrzmocd', 'no', 'dhzvitjizwpzggzm'] 1.41080999679e-37\n",
      "['sdkgjksopdawjosa', 'npdeopeiawjukjaxqahh', 'an'] 2.28573007724e-37\n",
      "['tel', 'hkltpqebxkptboqe', 'fpqfjbxkvlkbyrbiibo'] 2.38095770817e-37\n",
      "['ufmilmuqrfcylqucpr', 'fgqrgkcylwmlczscjjcp'] 1.30978270003e-37\n",
      "['vgnjmnvrsgdzmrvdqs', 'ghrshldzmxnmdatdkkdq'] 1.30978270003e-37\n",
      "['who', 'knows', 'the', 'answer', 'this', 'time', 'anyone', 'bueller'] 7.18422540159e-29\n",
      "['xiplopxtuifbotxfsu', 'ijtujnfbozpofcvfmmfs'] 1.30978270003e-37\n",
      "['yjqmpqyuvjgcpuygtv', 'jkuvkogcpaqpgdwgnngt'] 1.30978270003e-37\n",
      "['zkrnqrzvwkhdqvzhuw', 'klvwlphdqbrqhexhoohu'] 1.30978270003e-37\n",
      "['also', 'rsawxlierwaivxlm', 'wxmqiercsrifyippiv'] 6.27753124448e-35\n",
      "['bmtpstbxymjfsxbjwy', 'mnxynrjfsdtsjgzjqqjw'] 1.30978270003e-37\n",
      "['cnuqtucyznkgtyckxz', 'no', 'yzoskgteutkhakrrkx'] 1.41080999679e-37\n",
      "['do', 'vruvdzaolhuzdlya', 'opzaptlhufvuliblssly'] 1.43134393309e-37\n",
      "['epwsvweabpmivaemzb', 'pqabqumivgwvmjcmttmz'] 1.30978270003e-37\n",
      "['fqxtwxfbcqnjwbfnac', 'qrbcrvnjwhxwnkdnuuna'] 1.30978270003e-37\n",
      "['gryuxygcdrokxcgobd', 'rscdswokxiyxoleovvob'] 1.30978270003e-37\n",
      "['hszvyzhdesplydhp', 'cest', 'detxplyjzypmfpwwpc'] 4.70654751109e-37\n",
      "['it', 'awzaieftqmzeiqdf', 'tuefuyqmzkazqngqxxqd'] 4.23518038482e-37\n",
      "['jubxabjfgurnafjreg', 'uvfgvzrnalbarohryyre'] 1.30978270003e-37\n",
      "['kvcybckghvsobgksfh', 'vwghwasobmcbspiszzsf'] 1.30978270003e-37\n",
      "['lwdzcdlhiwtpchltgi', 'wxhixbtpcndctqjtaatg'] 1.30978270003e-37\n",
      "['mxeademijxuqdimuhj', 'xyijycuqdoedurkubbuh'] 1.30978270003e-37\n",
      "['nyfbefnjkyvrejnvik', 'yzjkzdvrepfevslvccvi'] 1.30978270003e-37\n",
      "['ozgcfgoklzwsfkowjl', 'zaklaewsfqgfwtmwddwj'] 1.30978270003e-37\n"
     ]
    }
   ],
   "source": [
    "candidates = [segment(rot(secret, i)) for i in range(len(alphabet))]\n",
    "\n",
    "for c in candidates:\n",
    "    print(c, Pwords(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about a general substitution cipher? The problem is that there are 26! substitution ciphers, and we can't enumerate all of them. We would need to search through this space. Initially make some guess at a substitution, then swap two letters; if that looks better keep going, if not try something else. This approach solves most substitution cipher problems, although it can take a few minutes on a message of length 100 words or so.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
